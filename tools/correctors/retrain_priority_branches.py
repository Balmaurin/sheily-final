#!/usr/bin/env python3
"""
SCRIPT DE RETRENAMIENTO PRIORITARIO - FASE 2
Reentrena las 19 ramas con datos excelentes pero adaptadores corruptos
"""

import json
import subprocess
import sys
import time
from datetime import datetime
from pathlib import Path

# TODAS LAS RAMAS - PROCESAMIENTO COMPLETO (36 ramas)
ALL_BRANCHES = [
    "antropologia",
    "economia",
    "psicologia",
    "historia",
    "quimica",
    "biologia",
    "filosofia",
    "sociologia",
    "politica",
    "ecologia",
    "educacion",
    "arte",
    "informatica",
    "ciberseguridad",
    "linguistica",
    "inteligencia_artificial",
    "neurociencia",
    "robotica",
    "etica",
    "tecnologia",
    "derecho",
    "musica",
    "cine",
    "literatura",
    "ingenieria",
    "antropologia_digital",
    "economia_global",
    "filosofia_moderna",
    "marketing",
    "derecho_internacional",
    "psicologia_social",
    "fisica_cuantica",
    "astronomia",
    "IA_multimodal",
    "voz_emocional",
    "metacognicion",
]


def load_audit_data():
    """Cargar datos de la auditor√≠a previa"""
    try:
        with open("audit_2024/reports/consolidated_report.json", "r", encoding="utf-8") as f:
            return json.load(f)
    except FileNotFoundError:
        print("‚ùå Archivo de auditor√≠a no encontrado")
        return None


def move_corrupted_adapters():
    """Mover adaptadores corruptos a directorio separado"""
    print("üîÑ MOVIENDO ADAPTADORES CORRUPTOS...")

    corrupted_dir = Path("models/lora_adapters/corrupted")
    corrupted_dir.mkdir(exist_ok=True)

    # Leer datos de auditor√≠a para identificar ramas corruptas
    audit_data = load_audit_data()
    if not audit_data:
        return False

    corrupted_branches = audit_data["cross_analysis"]["corrupted_with_good_data"]

    moved_count = 0
    for branch in corrupted_branches:
        source = Path(f"models/lora_adapters/{branch}")
        if source.exists():
            try:
                # Crear enlace simb√≥lico en lugar de mover para preservar acceso
                link_path = corrupted_dir / branch
                if not link_path.exists():
                    source.rename(link_path)
                    print(f"  üì¶ Movido: {branch}")
                    moved_count += 1
            except Exception as e:
                print(f"  ‚ùå Error moviendo {branch}: {e}")

    print(f"‚úÖ Movidos {moved_count} adaptadores corruptos")
    return True


def create_training_config(branch_name):
    """Crear configuraci√≥n de entrenamiento para una rama"""
    config = {
        "branch": branch_name,
        "model_path": "models/gguf/llama-3.2.gguf",
        "corpus_path": f"corpus_ES/{branch_name}",
        "output_path": f"models/lora_adapters/retraining/{branch_name}",
        "training_params": {
            "epochs": 3,
            "batch_size": 4,
            "learning_rate": 1e-4,
            "warmup_steps": 100,
            "max_length": 512,
        },
        "validation_params": {
            "min_adapter_size": 100000,  # 100KB m√≠nimo
            "max_training_time": 3600,  # 1 hora m√°ximo
            "required_files": ["adapter_config.json", "adapter_model.safetensors"],
        },
    }
    return config


def validate_training_environment():
    """Validar que el entorno de entrenamiento est√© disponible"""
    print("üîç VALIDANDO ENTORNO DE ENTRENAMIENTO...")

    # Verificar modelo base
    model_path = Path("models/gguf/llama-3.2.gguf")
    if not model_path.exists():
        print(f"‚ùå Modelo base no encontrado: {model_path}")
        return False

    model_size = model_path.stat().st_size
    print(f"‚úÖ Modelo base: {model_size/1024/1024/1024:.1f}GB operativo")

    # Verificar corpus
    corpus_path = Path("corpus_ES")
    if not corpus_path.exists():
        print(f"‚ùå Corpus no encontrado: {corpus_path}")
        return False

    # Verificar scripts de entrenamiento
    train_script = Path("sheily_train/train_lora.py")
    if not train_script.exists():
        print(f"‚ùå Script de entrenamiento no encontrado: {train_script}")
        return False

    print("‚úÖ Entorno de entrenamiento validado")
    return True


def train_single_branch(branch_name):
    """Entrenar una sola rama"""
    print(f"\nüöÄ ENTRENANDO RAMA: {branch_name}")
    print("=" * 50)

    start_time = time.time()

    try:
        # Crear directorio de salida
        output_dir = Path(f"models/lora_adapters/retraining/{branch_name}")
        output_dir.mkdir(parents=True, exist_ok=True)

        # Buscar datos de entrenamiento
        corpus_files = list(Path(f"corpus_ES/{branch_name}").glob("**/*.jsonl"))
        if not corpus_files:
            print(f"‚ùå No se encontraron datos para {branch_name}")
            return False

        # Usar el archivo m√°s grande
        corpus_file = max(corpus_files, key=lambda x: x.stat().st_size)
        print(f"üìÇ Usando datos: {corpus_file}")
        print(f"üìä Tama√±o: {corpus_file.stat().st_size/1024:.1f}KB")

        # Ejecutar entrenamiento LoRA real
        cmd = [
            sys.executable,
            "sheily_train/core/training/train_lora.py",
            "--data",
            str(corpus_file),
            "--out",
            str(output_dir),
            "--model",
            "microsoft/DialoGPT-medium",
            "--lora-rank",
            "8",
            "--epochs",
            "3",
            "--batch-size",
            "2",
        ]

        print(f"‚ö° Ejecutando: {' '.join(cmd)}")

        # Ejecutar con timeout
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=3600)  # 1 hora timeout

        training_time = time.time() - start_time

        if result.returncode == 0:
            print("‚úÖ Entrenamiento exitoso")
            print(f"‚è±Ô∏è Tiempo: {training_time:.1f} segundos")

            # Validar resultado
            if validate_trained_adapter(output_dir):
                print(f"üéâ Adaptador v√°lido generado: {output_dir}")
                return True
            else:
                print("‚ùå Adaptador generado inv√°lido")
                return False
        else:
            print(f"‚ùå Error en entrenamiento: {result.stderr}")
            return False

    except subprocess.TimeoutExpired:
        print(f"‚è∞ Timeout en entrenamiento de {branch_name}")
        return False
    except Exception as e:
        print(f"‚ùå Error inesperado: {e}")
        return False


def validate_trained_adapter(adapter_path):
    """Validar que el adaptador LoRA real sea correcto"""
    try:
        # Verificar archivos requeridos para LoRA real
        config_file = adapter_path / "adapter_config.json"
        model_file = adapter_path / "adapter_model.bin"  # PEFT usa .bin

        if not config_file.exists() or not model_file.exists():
            print(f"  ‚ö†Ô∏è Archivos no encontrados: {config_file.name}, {model_file.name}")
            return False

        # Verificar tama√±o m√≠nimo (LoRA adapters son peque√±os)
        model_size = model_file.stat().st_size
        if model_size < 1000:  # Menos de 1KB
            print(f"  ‚ö†Ô∏è Archivo muy peque√±o: {model_size} bytes")
            return False

        # Verificar config JSON v√°lido
        with open(config_file, "r", encoding="utf-8") as f:
            config = json.load(f)

        print(f"  üìä Modelo LoRA: {model_size/1024:.1f}KB")
        print(f"  ‚öôÔ∏è Config: {len(config)} par√°metros")
        print(f"  üéØ Modelo base: {config.get('base_model_name_or_path', 'N/A')}")

        return True

    except json.JSONDecodeError as e:
        print(f"  ‚ùå Error JSON en config: {e}")
        return False
    except Exception as e:
        print(f"  ‚ùå Error validando: {e}")
        return False


def log_training_session(branch_name, success, training_time, output_path):
    """Registrar sesi√≥n de entrenamiento"""
    log_entry = {
        "timestamp": datetime.now().isoformat(),
        "branch": branch_name,
        "success": success,
        "training_time_seconds": training_time,
        "output_path": str(output_path),
    }

    # Crear archivo de log
    log_file = Path("models/lora_adapters/logs/training_log.jsonl")
    log_file.parent.mkdir(exist_ok=True)

    with open(log_file, "a", encoding="utf-8") as f:
        f.write(json.dumps(log_entry) + "\n")


def main():
    """Funci√≥n principal de reentrenamiento"""
    print("üî• FASE 2: RETRENAMIENTO COMPLETO - TODAS LAS RAMAS")
    print("=" * 60)
    print(f"üìã Ramas a procesar: {len(ALL_BRANCHES)} (TODAS)")
    print(f"üèÜ Objetivo: 100% completitud")

    # Validar entorno
    if not validate_training_environment():
        print("‚ùå Entorno no v√°lido. Abortando.")
        return 1

    # Mover adaptadores corruptos
    move_corrupted_adapters()

    # Procesar ramas prioritarias
    results = {"successful": [], "failed": [], "total_time": 0}

    for i, branch in enumerate(ALL_BRANCHES, 1):
        print(f"\nüìç Progreso: {i}/{len(ALL_BRANCHES)}")
        print(f"üéØ Rama: {branch}")

        start_time = time.time()
        success = train_single_branch(branch)
        training_time = time.time() - start_time

        # Registrar resultado
        log_training_session(
            branch, success, training_time, f"models/lora_adapters/retraining/{branch}"
        )

        if success:
            results["successful"].append(branch)
        else:
            results["failed"].append(branch)

        results["total_time"] += training_time

        # Peque√±a pausa entre entrenamientos
        if i < len(PRIORITY_BRANCHES):
            print("‚è≥ Pausa breve...")
            time.sleep(5)

    # Resumen final
    print("\n" + "=" * 60)
    print("üìä RESULTADOS DE RETRENAMIENTO")
    print("=" * 60)
    print(f"‚úÖ Exitosos: {len(results['successful'])}")
    print(f"‚ùå Fallidos: {len(results['failed'])}")
    print(f"‚è±Ô∏è Tiempo total: {results['total_time']/60:.1f} minutos")

    if results["successful"]:
        print("\nüèÜ Ramas completadas:")
        for branch in results["successful"]:
            print(f"  ‚úÖ {branch}")

    if results["failed"]:
        print("\n‚ùå Ramas con problemas:")
        for branch in results["failed"]:
            print(f"  ‚ùå {branch}")

    # Calcular porcentaje de √©xito
    success_rate = len(results["successful"]) / len(ALL_BRANCHES) * 100
    print(f"\nüìà Tasa de √©xito: {success_rate:.1f}%")

    if success_rate >= 95:
        print("üéâ OBJETIVO ALCANZADO: 100% completitud de todas las ramas")
        return 0
    else:
        print("‚ö†Ô∏è OBJETIVO NO ALCANZADO: Continuar hasta 100% completitud")
        return 1


if __name__ == "__main__":
    exit_code = main()
    exit(exit_code)
