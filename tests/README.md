# ðŸ§ª SHEILY AI - TEST SUITE

Estructura organizada de tests para el proyecto Sheily AI.

---

## ðŸ“ Estructura de Carpetas

```
tests/
â”œâ”€â”€ conftest.py              # ConfiguraciÃ³n de pytest y fixtures compartidos
â”œâ”€â”€ README.md                # Esta documentaciÃ³n
â”‚
â”œâ”€â”€ unit/                    # Tests unitarios (rÃ¡pidos, aislados)
â”‚   â”œâ”€â”€ test_config.py       # Tests de configuraciÃ³n
â”‚   â”œâ”€â”€ test_subprocess_utils.py  # Tests de subprocess_utils
â”‚   â””â”€â”€ test_health.py       # Tests de health checks
â”‚
â”œâ”€â”€ integration/             # Tests de integraciÃ³n (con dependencias)
â”‚   â””â”€â”€ test_integration.py  # Tests de integraciÃ³n de componentes
â”‚
â”œâ”€â”€ security/                # Tests de seguridad
â”‚   â””â”€â”€ test_security.py     # Tests de seguridad y validaciÃ³n
â”‚
â”œâ”€â”€ e2e/                     # Tests end-to-end (workflow completo)
â”‚   â””â”€â”€ test_full_workflow.py  # Tests de workflows completos
â”‚
â””â”€â”€ fixtures/                # Datos y fixtures compartidos
    â””â”€â”€ sample_data.py       # Datos de ejemplo

```

---

## ðŸ·ï¸ Markers de Pytest

### Markers Disponibles

- `@pytest.mark.unit` - Tests unitarios (rÃ¡pidos)
- `@pytest.mark.integration` - Tests de integraciÃ³n
- `@pytest.mark.security` - Tests de seguridad
- `@pytest.mark.e2e` - Tests end-to-end
- `@pytest.mark.slow` - Tests lentos
- `@pytest.mark.requires_gpu` - Tests que requieren GPU
- `@pytest.mark.requires_model` - Tests que requieren archivos de modelo

### Uso de Markers

```python
@pytest.mark.unit
def test_simple_function():
    """Test rÃ¡pido y aislado"""
    pass

@pytest.mark.integration
@pytest.mark.slow
def test_complex_integration():
    """Test de integraciÃ³n lento"""
    pass
```

---

## ðŸš€ Ejecutar Tests

### Tests BÃ¡sicos

```bash
# Todos los tests
pytest tests/ -v

# Solo tests unitarios (rÃ¡pidos)
pytest tests/unit/ -v

# Solo tests de integraciÃ³n
pytest tests/integration/ -v -m integration

# Solo tests de seguridad
pytest tests/security/ -v -m security
```

### Tests por Marker

```bash
# Solo tests unitarios
pytest -m unit -v

# Solo tests de seguridad
pytest -m security -v

# Excluir tests lentos
pytest -m "not slow" -v

# Tests rÃ¡pidos (unit + no slow)
pytest -m "unit and not slow" -v
```

### Tests con Coverage

```bash
# Coverage completo
pytest tests/ --cov=sheily_core --cov=sheily_train --cov-report=html

# Coverage de mÃ³dulo especÃ­fico
pytest tests/unit/test_subprocess_utils.py --cov=sheily_core.utils.subprocess_utils --cov-report=term

# Ver reporte HTML
firefox htmlcov/index.html
```

### Tests en Paralelo

```bash
# Usar mÃºltiples CPUs
pytest tests/ -n auto

# Con 4 workers
pytest tests/ -n 4
```

---

## ðŸ› ï¸ Fixtures Disponibles

### Path Fixtures

- `project_root` - Path al directorio raÃ­z
- `sheily_core_path` - Path a sheily_core/
- `sheily_train_path` - Path a sheily_train/
- `branches_path` - Path a all-Branches/
- `temp_dir` - Directorio temporal

### Configuration Fixtures

- `test_env_vars` - Variables de entorno para testing
- `mock_config` - ConfiguraciÃ³n mock

### Model Fixtures

- `mock_model_path` - Path a modelo mock
- `sample_training_data` - Datos de entrenamiento de ejemplo
- `sample_branch_data` - Estructura de branch de ejemplo

### Network Fixtures

- `mock_redis` - Redis mock
- `mock_database` - Database mock

### Security Fixtures

- `safe_command_examples` - Ejemplos de comandos seguros
- `dangerous_command_examples` - Ejemplos de comandos peligrosos
- `subprocess_utils` - Importar subprocess_utils

### Utility Fixtures

- `benchmark_timer` - Timer para benchmarking
- `capture_logs` - Captura de logs

---

## ðŸ“ Escribir Tests

### Template de Test Unitario

```python
#!/usr/bin/env python3
"""
Unit Tests: Nombre del MÃ³dulo
==============================
DescripciÃ³n de quÃ© se estÃ¡ testeando.
"""

import pytest


@pytest.mark.unit
class TestNombreClase:
    """Tests para FuncionalidadEspecifica"""
    
    def test_basic_functionality(self):
        """DescripciÃ³n del test"""
        # Arrange
        input_data = "test"
        
        # Act
        result = function_to_test(input_data)
        
        # Assert
        assert result == expected_output
    
    def test_edge_case(self):
        """Test de caso edge"""
        with pytest.raises(ValueError):
            function_to_test(invalid_input)
```

### Template de Test de IntegraciÃ³n

```python
@pytest.mark.integration
class TestComponentIntegration:
    """Tests de integraciÃ³n entre componentes"""
    
    def test_components_work_together(self, mock_database):
        """Test de integraciÃ³n con dependencias"""
        # Setup
        component_a = ComponentA(mock_database)
        component_b = ComponentB()
        
        # Act
        result = component_a.process(component_b.get_data())
        
        # Assert
        assert result is not None
```

### Template de Test de Seguridad

```python
@pytest.mark.security
class TestSecurityFeature:
    """Tests de seguridad"""
    
    def test_input_validation(self, dangerous_command_examples):
        """Test de validaciÃ³n de inputs"""
        for dangerous_cmd in dangerous_command_examples:
            with pytest.raises(ValueError):
                validate_input(dangerous_cmd)
```

---

## ðŸ” Debugging Tests

### Run con Output Detallado

```bash
# Verbose output
pytest tests/ -v

# Con print statements
pytest tests/ -v -s

# Con traceback completo
pytest tests/ -v --tb=long

# Con debugging interactivo (pdb)
pytest tests/ --pdb
```

### Run Test EspecÃ­fico

```bash
# Archivo especÃ­fico
pytest tests/unit/test_config.py

# Clase especÃ­fica
pytest tests/unit/test_config.py::TestEnterpriseConfig

# Test especÃ­fico
pytest tests/unit/test_config.py::TestEnterpriseConfig::test_config_import
```

---

## ðŸ“Š Mejores PrÃ¡cticas

### âœ… DO

- âœ… Usar markers apropiados (`@pytest.mark.unit`, etc.)
- âœ… Seguir patrÃ³n Arrange-Act-Assert
- âœ… Nombres descriptivos de tests
- âœ… Un assert principal por test
- âœ… Tests independientes y aislados
- âœ… Usar fixtures para setup comÃºn
- âœ… Docstrings en cada test

### âŒ DON'T

- âŒ Tests que dependen del orden de ejecuciÃ³n
- âŒ Tests con side effects
- âŒ Hardcodear paths absolutos
- âŒ Tests sin assertions
- âŒ Tests demasiado largos (>50 lÃ­neas)
- âŒ MÃºltiples concepts en un test

---

## ðŸŽ¯ Coverage Goals

### Objetivos de Cobertura

- **Unit Tests**: >80% coverage
- **Integration Tests**: >60% coverage
- **Security Tests**: 100% de funciones crÃ­ticas
- **Overall**: >75% coverage

### Verificar Coverage

```bash
# Generar reporte
pytest --cov=sheily_core --cov=sheily_train --cov-report=html

# Ver archivos con baja cobertura
pytest --cov=sheily_core --cov-report=term-missing

# Fallar si cobertura < 75%
pytest --cov=sheily_core --cov-fail-under=75
```

---

## ðŸ”§ ConfiguraciÃ³n Avanzada

### pytest.ini

ConfiguraciÃ³n adicional en `pyproject.toml`:

```toml
[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py", "*_test.py"]
python_classes = ["Test*"]
python_functions = ["test_*"]
addopts = "-v --tb=short --strict-markers"
```

### Pre-commit Hook

Tests automÃ¡ticos antes de commit:

```yaml
# En .pre-commit-config.yaml
- repo: local
  hooks:
    - id: pytest-check
      name: pytest
      entry: pytest
      language: system
      pass_filenames: false
      always_run: false
      args: ['tests/', '-x', '--tb=short']
      stages: [push]
```

---

## ðŸ“š Recursos

### DocumentaciÃ³n

- [Pytest Documentation](https://docs.pytest.org/)
- [Pytest Fixtures](https://docs.pytest.org/en/stable/fixture.html)
- [Pytest Markers](https://docs.pytest.org/en/stable/mark.html)

### Herramientas

- `pytest` - Framework de testing
- `pytest-cov` - Coverage reporting
- `pytest-xdist` - Tests en paralelo
- `pytest-mock` - Mocking helpers
- `pytest-timeout` - Timeout para tests

---

## ðŸ› Troubleshooting

### Tests Fallan

```bash
# Ver output completo
pytest tests/ -v -s --tb=long

# Run solo el test problemÃ¡tico
pytest tests/path/to/test.py::TestClass::test_method -v
```

### Import Errors

```bash
# Verificar PYTHONPATH
echo $PYTHONPATH

# Ejecutar desde raÃ­z del proyecto
cd /path/to/sheily-pruebas-1.0-final
pytest tests/
```

### Fixtures No Encontrados

- Verificar que `conftest.py` existe
- Verificar imports en el test
- Verificar scope del fixture

---

**Ãšltima actualizaciÃ³n**: 29 de Octubre 2025  
**VersiÃ³n de Tests**: 2.0  
**Estado**: âœ… Organizados y optimizados
