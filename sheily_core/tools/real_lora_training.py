#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
IMPLEMENTACIÃ“N REAL DE ENTRENAMIENTO LoRA
========================================
Script para entrenamiento efectivo de adaptadores LoRA acadÃ©micos.
Sustituye las simulaciones con implementaciÃ³n tÃ©cnica real.
"""

import json
import os
import sys
import time
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import torch
import torch.nn as nn

# Importar bibliotecas reales de ML
try:
    import accelerate
    import transformers
    from datasets import Dataset
    from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
    from transformers import (
        AutoModelForCausalLM,
        AutoTokenizer,
        DataCollatorForLanguageModeling,
        Trainer,
        TrainingArguments,
    )

    ML_AVAILABLE = True
    print("âœ… Bibliotecas de ML disponibles")

except ImportError as e:
    print(f"âŒ Error importando bibliotecas de ML: {e}")
    print("Instalando dependencias necesarias...")
    ML_AVAILABLE = False


@dataclass
class TrainingConfig:
    """ConfiguraciÃ³n de entrenamiento LoRA"""

    model_name: str = "microsoft/DialoGPT-medium"
    dataset_name: str = "bookcorpus"
    output_dir: str = "./models/lora_adapters/real_training"
    num_train_epochs: int = 3
    per_device_train_batch_size: int = 2
    per_device_eval_batch_size: int = 2
    learning_rate: float = 1e-4
    warmup_steps: int = 100
    weight_decay: float = 0.01
    logging_steps: int = 50
    save_steps: int = 500
    evaluation_strategy: str = "steps"
    eval_steps: int = 250
    lora_r: int = 16
    lora_alpha: int = 32
    lora_dropout: float = 0.1
    target_modules: List[str] = None

    def __post_init__(self):
        if self.target_modules is None:
            self.target_modules = ["c_attn"]


class RealLoraTrainer:
    """Entrenador LoRA real para especializaciones acadÃ©micas"""

    def __init__(self, config: TrainingConfig):
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ðŸ”§ Usando dispositivo: {self.device}")

        if not ML_AVAILABLE:
            raise ImportError("Bibliotecas de ML no disponibles. Ejecute instalaciÃ³n primero.")

    def load_model_and_tokenizer(self) -> Tuple[AutoModelForCausalLM, AutoTokenizer]:
        """Cargar modelo base y tokenizador"""
        print(f"ðŸ“¥ Cargando modelo base: {self.config.model_name}")

        try:
            tokenizer = AutoTokenizer.from_pretrained(self.config.model_name)
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token

            model = AutoModelForCausalLM.from_pretrained(
                self.config.model_name,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                device_map="auto" if torch.cuda.is_available() else None,
            )

            # Preparar modelo para LoRA
            model = prepare_model_for_kbit_training(model)

            print("âœ… Modelo y tokenizador cargados correctamente")
            return model, tokenizer

        except Exception as e:
            print(f"âŒ Error cargando modelo: {e}")
            # Usar modelo alternativo mÃ¡s pequeÃ±o para pruebas
            print("ðŸ”„ Usando modelo alternativo mÃ¡s pequeÃ±o...")
            return self._load_smaller_model()

    def _load_smaller_model(self) -> Tuple[AutoModelForCausalLM, AutoTokenizer]:
        """Cargar modelo mÃ¡s pequeÃ±o para pruebas"""
        try:
            model_name = "distilgpt2"
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            tokenizer.pad_token = tokenizer.eos_token

            model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float32)

            return model, tokenizer

        except Exception as e:
            raise Exception(f"No se pudo cargar ningÃºn modelo: {e}")

    def prepare_lora_config(self, model: AutoModelForCausalLM) -> LoraConfig:
        """Preparar configuraciÃ³n LoRA"""
        print("ðŸ”§ Configurando parÃ¡metros LoRA...")
        # Detectar mÃ³dulos objetivo automÃ¡ticamente
        target_modules = []
        for name, module in model.named_modules():
            if any(target in name.lower() for target in ["attn", "attention", "proj"]):
                if "c_attn" in name or "c_proj" in name:
                    target_modules.append(name.split(".")[-1])

        if not target_modules:
            target_modules = ["c_attn"]

        print(f"ðŸŽ¯ MÃ³dulos objetivo detectados: {target_modules}")

        lora_config = LoraConfig(
            r=self.config.lora_r,
            lora_alpha=self.config.lora_alpha,
            lora_dropout=self.config.lora_dropout,
            target_modules=target_modules,
            bias="none",
            task_type="CAUSAL_LM",
        )

        return lora_config

    def prepare_dataset(self, academic_branch: str) -> Dataset:
        """Preparar dataset para rama acadÃ©mica especÃ­fica"""
        print(f"ðŸ“š Preparando dataset para rama: {academic_branch}")

        # Crear datos de entrenamiento especializados
        training_texts = self._generate_academic_training_corpus(academic_branch)

        # Crear dataset
        dataset = Dataset.from_dict({"text": training_texts})

        print(f"âœ… Dataset creado con {len(training_texts)} ejemplos")
        return dataset

    def _generate_academic_training_corpus(self, branch: str) -> List[str]:
        """Generar datos de entrenamiento acadÃ©micos especializados"""
        # Datos especializados por rama acadÃ©mica
        academic_data = {
            "antropologia": [
                "La antropologÃ­a cultural estudia las variaciones culturales humanas y su impacto en el comportamiento social.",
                "Los antropÃ³logos analizan cÃ³mo las sociedades desarrollan normas, valores y prÃ¡cticas culturales Ãºnicas.",
                "El estudio antropolÃ³gico revela cÃ³mo la cultura influye en la percepciÃ³n del mundo y las relaciones sociales.",
            ],
            "economia": [
                "La teorÃ­a econÃ³mica explica cÃ³mo los mercados asignan recursos escasos entre necesidades ilimitadas.",
                "Los modelos macroeconÃ³micos analizan el crecimiento econÃ³mico y los ciclos de negocio.",
                "La economÃ­a del comportamiento integra psicologÃ­a y economÃ­a para entender decisiones individuales.",
            ],
            "psicologia": [
                "La psicologÃ­a cognitiva estudia los procesos mentales internos como la memoria y el pensamiento.",
                "Las teorÃ­as psicolÃ³gicas explican cÃ³mo los factores ambientales influyen en el comportamiento humano.",
                "La neuropsicologÃ­a investiga la relaciÃ³n entre el cerebro y los procesos psicolÃ³gicos.",
            ],
        }

        # Obtener datos especÃ­ficos o datos generales
        if branch in academic_data:
            base_texts = academic_data[branch]
        else:
            base_texts = [
                f"El estudio acadÃ©mico de {branch} proporciona conocimientos especializados en esta disciplina.",
                f"Los expertos en {branch} desarrollan teorÃ­as y metodologÃ­as especÃ­ficas para su campo.",
                f"La investigaciÃ³n en {branch} contribuye al avance del conocimiento humano.",
            ]

        # Expandir datos para entrenamiento
        expanded_texts = []
        for text in base_texts:
            # Crear variaciones del texto
            expanded_texts.append(text)
            expanded_texts.append(f"En el contexto acadÃ©mico: {text}")
            expanded_texts.append(f"Desde la perspectiva de {branch}: {text}")

        return expanded_texts[:50]  # Limitar para pruebas iniciales

    def train_adapter(self, academic_branch: str) -> Dict:
        """Entrenar adaptador LoRA para rama acadÃ©mica especÃ­fica"""
        print(f"ðŸš€ Iniciando entrenamiento real para rama: {academic_branch}")

        start_time = time.time()

        try:
            # 1. Cargar modelo y tokenizador
            model, tokenizer = self.load_model_and_tokenizer()

            # 2. Preparar configuraciÃ³n LoRA
            lora_config = self.prepare_lora_config(model)

            # 3. Aplicar LoRA al modelo
            model = get_peft_model(model, lora_config)
            model.print_trainable_parameters()

            # 4. Preparar dataset
            dataset = self.prepare_dataset(academic_branch)

            # Tokenizar dataset
            def tokenize_function(examples):
                return tokenizer(
                    examples["text"], truncation=True, padding="max_length", max_length=512
                )

            tokenized_dataset = dataset.map(tokenize_function, batched=True)

            # 5. Configurar entrenamiento
            output_dir = Path(self.config.output_dir) / academic_branch
            output_dir.mkdir(parents=True, exist_ok=True)

            training_args = TrainingArguments(
                output_dir=str(output_dir),
                num_train_epochs=self.config.num_train_epochs,
                per_device_train_batch_size=self.config.per_device_train_batch_size,
                per_device_eval_batch_size=self.config.per_device_eval_batch_size,
                learning_rate=self.config.learning_rate,
                warmup_steps=self.config.warmup_steps,
                weight_decay=self.config.weight_decay,
                logging_dir=f"{output_dir}/logs",
                logging_steps=self.config.logging_steps,
                save_steps=self.config.save_steps,
                evaluation_strategy=self.config.evaluation_strategy,
                eval_steps=self.config.eval_steps,
                save_total_limit=3,
                load_best_model_at_end=True,
                report_to=["tensorboard"],
            )

            # 6. Configurar entrenador
            data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

            trainer = Trainer(
                model=model,
                args=training_args,
                train_dataset=tokenized_dataset,
                eval_dataset=tokenized_dataset,  # Usar mismo dataset para evaluaciÃ³n bÃ¡sica
                data_collator=data_collator,
            )

            # 7. Ejecutar entrenamiento
            print("ðŸŽ¯ Iniciando entrenamiento...")
            trainer.train()

            # 8. Guardar modelo entrenado
            trainer.save_model(str(output_dir))

            # 9. Crear metadatos reales
            training_time = time.time() - start_time

            metadata = {
                "academic_branch": academic_branch,
                "training_timestamp": datetime.now().isoformat(),
                "training_duration": round(training_time, 2),
                "model_base": self.config.model_name,
                "lora_config": {
                    "r": self.config.lora_r,
                    "alpha": self.config.lora_alpha,
                    "dropout": self.config.lora_dropout,
                },
                "dataset_info": {
                    "samples": len(tokenized_dataset),
                    "specialization": academic_branch,
                },
                "training_config": {
                    "epochs": self.config.num_train_epochs,
                    "batch_size": self.config.per_device_train_batch_size,
                    "learning_rate": self.config.learning_rate,
                },
                "hardware_used": {
                    "device": str(self.device),
                    "gpu_available": torch.cuda.is_available(),
                    "gpu_name": torch.cuda.get_device_name(0)
                    if torch.cuda.is_available()
                    else "N/A",
                },
                "status": "trained",
                "version": "2.0.0",
            }

            # Guardar metadatos reales
            with open(output_dir / "training_metadata.json", "w", encoding="utf-8") as f:
                json.dump(metadata, f, indent=2, ensure_ascii=False)

            print(f"âœ… Adaptador {academic_branch} entrenado correctamente en {training_time:.2f}s")

            return {
                "branch": academic_branch,
                "status": "SUCCESS",
                "training_time": training_time,
                "output_path": str(output_dir),
                "model_size": sum(p.numel() for p in model.parameters() if p.requires_grad),
            }

        except Exception as e:
            print(f"âŒ Error en entrenamiento de {academic_branch}: {e}")
            return {"branch": academic_branch, "status": "ERROR", "error": str(e)}


def main():
    """FunciÃ³n principal de entrenamiento real"""
    print("ðŸš€ ENTRENAMIENTO REAL DE ADAPTADORES LoRA ACADÃ‰MICOS")
    print("=" * 60)

    if not ML_AVAILABLE:
        print("âŒ Bibliotecas de ML no disponibles")
        print("ðŸ’¡ Ejecute: pip install -r requirements_ml.txt")
        return 1

    # ConfiguraciÃ³n de entrenamiento
    config = TrainingConfig()

    # Crear entrenador
    trainer = RealLoraTrainer(config)

    # Ramas acadÃ©micas a procesar (primeras 3 para prueba inicial)
    test_branches = ["antropologia", "economia", "psicologia"]

    results = []
    for branch in test_branches:
        print(f"\nðŸŽ¯ Procesando rama: {branch}")
        result = trainer.train_adapter(branch)
        results.append(result)

    # Reporte final
    print("\n" + "=" * 60)
    print("ðŸ“Š REPORTE DE ENTRENAMIENTO REAL")
    print("=" * 60)

    successful = sum(1 for r in results if r["status"] == "SUCCESS")
    print(f"âœ… Ã‰xitos: {successful}/{len(results)}")

    for result in results:
        if result["status"] == "SUCCESS":
            print(f"   âœ… {result['branch']}: {result['training_time']:.1f}s")
        else:
            print(f"   âŒ {result['branch']}: {result.get('error', 'Error desconocido')}")

    return 0 if successful > 0 else 1


if __name__ == "__main__":
    exit(main())
