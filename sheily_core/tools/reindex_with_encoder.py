#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SHEILY REINDEXER ZERO-DEPENDENCY
Script para reindexar embeddings sin dependencias externas

DEPENDENCIAS: Solo Python stdlib
ALTERNATIVA: Para embeddings reales, usar rag_zero_deps.py con TF-IDF
"""
import argparse
import json
import math
import sqlite3
from pathlib import Path


def create_simple_embeddings(texts, dims=384):
    """
    Crea embeddings bÃ¡sicos usando hash y caracterÃ­sticas del texto
    No es tan avanzado como sentence-transformers, pero funciona sin dependencias
    """
    embeddings = []

    for text in texts:
        # Vector base usando hash del texto
        text_hash = hash(text.lower()) % (2**31)

        # CaracterÃ­sticas del texto
        features = {
            "length": len(text),
            "words": len(text.split()),
            "chars": len(set(text.lower())),
            "digits": sum(c.isdigit() for c in text),
            "upper": sum(c.isupper() for c in text),
            "spaces": text.count(" "),
            "punctuation": sum(c in ".,!?;:" for c in text),
        }

        # Crear vector de dimensiÃ³n fija
        vector = []
        for i in range(dims):
            # Combinar hash con caracterÃ­sticas
            seed = (text_hash + i) % (2**31)

            # Normalizar caracterÃ­sticas
            feature_sum = sum(features.values())
            norm_features = feature_sum / (1 + i % 10)

            # Valor del vector
            value = math.sin(seed + norm_features) * math.cos(i)
            vector.append(value)

        # Normalizar vector (simulando normalize_embeddings=True)
        magnitude = math.sqrt(sum(x * x for x in vector))
        if magnitude > 0:
            vector = [x / magnitude for x in vector]

        embeddings.append(vector)

    return embeddings


def save_embeddings_sqlite(vectors_path, embeddings, ids):
    """Guardar embeddings en SQLite en lugar de numpy"""
    db_path = str(vectors_path).replace(".npz", ".db")

    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()

    # Crear tabla
    cursor.execute(
        """
        CREATE TABLE IF NOT EXISTS embeddings (
            id TEXT PRIMARY KEY,
            vector TEXT,
            dimension INTEGER
        )
    """
    )

    # Insertar embeddings
    for i, (embedding, doc_id) in enumerate(zip(embeddings, ids)):
        vector_json = json.dumps(embedding)
        cursor.execute(
            "INSERT OR REPLACE INTO embeddings (id, vector, dimension) VALUES (?, ?, ?)",
            (str(doc_id), vector_json, len(embedding)),
        )

    conn.commit()
    conn.close()

    print(f"âœ… Embeddings guardados en SQLite: {db_path}")
    return db_path


def main():
    print("ğŸ”„ SHEILY REINDEXER ZERO-DEPENDENCY")
    print("=" * 50)

    ap = argparse.ArgumentParser(description="Reindexar con embeddings bÃ¡sicos (zero-dependency)")
    ap.add_argument("--run_dir", required=True, help="branches/<rama>/<fecha>")
    ap.add_argument(
        "--encoder_path", default="basic_hash", help="Tipo de encoder (basic_hash, tfidf)"
    )
    ap.add_argument("--dimensions", type=int, default=384, help="Dimensiones del vector")
    args = ap.parse_args()

    run = Path(args.run_dir)
    texts_path = run / "st" / "texts.jsonl"
    vectors_path = run / "st" / "vectors.npz"
    meta_path = run / "st" / "meta.json"
    index_path = run / "st" / "index.json"

    if not texts_path.exists():
        print(f"âŒ Falta {texts_path}")
        return

    # Leer textos
    ids, texts = [], []
    with texts_path.open("r", encoding="utf-8") as f:
        for line in f:
            j = json.loads(line)
            ids.append(j["id"])
            texts.append(j["text"])

    print(f"ğŸ“„ Procesando {len(texts)} documentos")
    print(f"ğŸ”§ Encoder: {args.encoder_path} (dimensiones: {args.dimensions})")

    # Crear embeddings bÃ¡sicos
    if args.encoder_path == "tfidf":
        # Usar el sistema TF-IDF de rag_zero_deps.py
        print("âš ï¸  Para embeddings TF-IDF avanzados, usar: python scripts/rag_zero_deps.py")
        print("ğŸ“ Usando embeddings bÃ¡sicos por hash...")

    embeddings = create_simple_embeddings(texts, args.dimensions)

    # Guardar embeddings en SQLite
    db_path = save_embeddings_sqlite(vectors_path, embeddings, ids)

    # Crear metadata
    meta = {
        "mode": "zero_deps",
        "vector_model": f"basic_embeddings_{args.encoder_path}",
        "dims": args.dimensions,
        "total_chunks": len(embeddings),
        "method": "hash_based_features",
        "database": db_path,
    }
    meta_path.write_text(json.dumps(meta, ensure_ascii=False, indent=2), "utf-8")

    # Crear Ã­ndice si no existe
    if not index_path.exists():
        index_data = {"ids": ids, "method": "zero_dependency", "embedding_type": "basic_hash"}
        index_path.write_text(json.dumps(index_data, ensure_ascii=False, indent=2), "utf-8")

    print(f"âœ… Reindex completado:")
    print(f"   ğŸ“Š Metadata: {meta_path}")
    print(f"   ğŸ—ƒï¸  Base datos: {db_path}")
    print(f"   ğŸ“‘ Ãndice: {index_path}")
    print("\nğŸ’¡ Para bÃºsqueda semÃ¡ntica avanzada, usar:")
    print("   python scripts/rag_zero_deps.py --action search --query 'tu consulta'")


if __name__ == "__main__":
    main()
