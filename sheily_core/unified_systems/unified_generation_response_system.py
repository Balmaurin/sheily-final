#!/usr/bin/env python3
"""
Sistema Unificado de Generaci√≥n y Respuesta

Este m√≥dulo combina funcionalidades de:
- Generation Output (generation_output.py)
- Response Orchestrator (response_orchestrator.py)
- Semantic Validator (semantic_validator.py)
- Prompt Adaptativo (prompt_adaptativo.py)
- Generacion Adaptativa (generacion_adaptativa.py)
- Refinamiento Semantico (refinamiento_semantico.py)
- Classification Output (classification_output.py)
- Regression Output (regression_output.py)
"""

import asyncio
import json
import logging
import sqlite3
import time
from collections import defaultdict, deque
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple, Union

import numpy as np

# Importaci√≥n segura de PyTorch
try:
    import torch
    import torch.nn as nn
    from sentence_transformers import SentenceTransformer

    # Verificaci√≥n simple sin acceder a internos
    if hasattr(torch, "__version__"):
        TORCH_AVAILABLE = True
    else:
        TORCH_AVAILABLE = False
except Exception as e:
    print(f"‚ö†Ô∏è PyTorch/Transformers no disponible: {e}")
    TORCH_AVAILABLE = False
    torch = None
    nn = None
    SentenceTransformer = None
import difflib

# Configurar logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class GenerationType(Enum):
    # ...existing code...

    TEXT = "text"
    CLASSIFICATION = "classification"
    REGRESSION = "regression"
    CODE = "code"
    CREATIVE = "creative"
    TECHNICAL = "technical"


class ResponseMode(Enum):
    # ...existing code...

    DIRECT = "direct"
    ADAPTIVE = "adaptive"
    CONTEXTUAL = "contextual"
    CREATIVE = "creative"
    ANALYTICAL = "analytical"


class ValidationLevel(Enum):
    # ...existing code...

    BASIC = "basic"
    SEMANTIC = "semantic"
    LOGICAL = "logical"
    COMPREHENSIVE = "comprehensive"


@dataclass
class GenerationConfig:
    # ...existing code...

    generation_type: GenerationType = GenerationType.TEXT
    response_mode: ResponseMode = ResponseMode.ADAPTIVE
    validation_level: ValidationLevel = ValidationLevel.SEMANTIC
    max_length: int = 1000
    temperature: float = 0.7
    creativity_level: float = 0.5
    context_window: int = 10
    quality_threshold: float = 0.7
    enable_adaptation: bool = True
    enable_refinement: bool = True


@dataclass
class GenerationRequest:
    # ...existing code...

    prompt: str
    context: Optional[Dict[str, Any]] = None
    generation_type: GenerationType = GenerationType.TEXT
    response_mode: ResponseMode = ResponseMode.ADAPTIVE
    constraints: Optional[Dict[str, Any]] = None
    metadata: Optional[Dict[str, Any]] = None


@dataclass
class GenerationResult:
    # ...existing code...

    id: str
    content: str
    generation_type: GenerationType
    response_mode: ResponseMode
    quality_score: float
    confidence: float
    processing_time: float
    created_at: datetime
    metadata: Dict[str, Any] = field(default_factory=dict)
    validation_results: Optional[Dict[str, Any]] = None
    refinements: List[str] = field(default_factory=list)


@dataclass
class ValidationResult:
    # ...existing code...

    is_valid: bool
    score: float
    issues: List[str]
    suggestions: List[str]
    confidence: float
    validation_level: ValidationLevel


class UnifiedGenerationResponseSystem:
    # ...existing code...

    def __init__(self, config: Optional[GenerationConfig] = None, db_path: Optional[str] = None):
        """Inicializar sistema unificado"""
        self.config = config or GenerationConfig()
        self.db_path = db_path or "./data/generation_response_system.db"

        # Componentes del sistema
        self.generation_history: List[GenerationResult] = []
        self.prompt_templates: Dict[str, str] = {}
        self.quality_metrics: Dict[str, List[float]] = defaultdict(list)
        self.adaptation_patterns: Dict[str, Any] = {}
        self.components: Dict[str, Any] = {}  # Para modelos especializados

        # Inicializar componentes
        self._init_database()
        self._init_generation_components()
        self._init_validation_components()
        self._init_adaptation_components()

        logger.info("‚úÖ Sistema Unificado de Generaci√≥n y Respuesta inicializado")

    def _init_database(self):
        """Inicializar base de datos"""
        try:
            # Crear directorio si no existe
            Path(self.db_path).parent.mkdir(parents=True, exist_ok=True)
            self.conn = sqlite3.connect(self.db_path)
            self._create_tables()
            logger.info("‚úÖ Base de datos de generaci√≥n y respuesta inicializada")
        except Exception as e:
            logger.error(f"Error inicializando base de datos: {e}")
            raise

    def _create_tables(self):
        """Crear tablas en base de datos"""
        cursor = self.conn.cursor()

        # Tabla de generaciones
        cursor.execute(
            """
            CREATE TABLE IF NOT EXISTS generations (
                id TEXT PRIMARY KEY,
                prompt TEXT NOT NULL,
                content TEXT NOT NULL,
                generation_type TEXT NOT NULL,
                response_mode TEXT NOT NULL,
                quality_score REAL NOT NULL,
                confidence REAL NOT NULL,
                processing_time REAL NOT NULL,
                created_at TEXT NOT NULL,
                metadata TEXT,
                validation_results TEXT
            )
        """
        )

        # Tabla de validaciones
        cursor.execute(
            """
            CREATE TABLE IF NOT EXISTS validations (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                generation_id TEXT NOT NULL,
                is_valid BOOLEAN NOT NULL,
                score REAL NOT NULL,
                issues TEXT,
                suggestions TEXT,
                confidence REAL NOT NULL,
                validation_level TEXT NOT NULL,
                timestamp TEXT NOT NULL,
                FOREIGN KEY (generation_id) REFERENCES generations (id)
            )
        """
        )

        # Tabla de refinamientos
        cursor.execute(
            """
            CREATE TABLE IF NOT EXISTS refinements (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                generation_id TEXT NOT NULL,
                original_content TEXT NOT NULL,
                refined_content TEXT NOT NULL,
                improvement_score REAL NOT NULL,
                refinement_type TEXT NOT NULL,
                timestamp TEXT NOT NULL,
                FOREIGN KEY (generation_id) REFERENCES generations (id)
            )
        """
        )

        # Tabla de m√©tricas de calidad
        cursor.execute(
            """
            CREATE TABLE IF NOT EXISTS quality_metrics (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                metric_type TEXT NOT NULL,
                metric_value REAL NOT NULL,
                generation_id TEXT,
                timestamp TEXT NOT NULL,
                metadata TEXT
            )
        """
        )

        # √çndices
        cursor.execute(
            "CREATE INDEX IF NOT EXISTS idx_generation_type ON generations(generation_type)"
        )
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_quality_score ON generations(quality_score)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_created_at ON generations(created_at)")
        cursor.execute(
            "CREATE INDEX IF NOT EXISTS idx_validation_level ON validations(validation_level)"
        )

        self.conn.commit()
        cursor.close()

    def _init_generation_components(self):
        """Inicializar componentes de generaci√≥n"""
        self.generation_types = {
            GenerationType.TEXT: self._generate_text,
            GenerationType.CLASSIFICATION: self._generate_classification,
            GenerationType.REGRESSION: self._generate_regression,
            GenerationType.CODE: self._generate_code,
            GenerationType.CREATIVE: self._generate_creative,
            GenerationType.TECHNICAL: self._generate_technical,
        }

        self.response_modes = {
            ResponseMode.DIRECT: self._direct_response,
            ResponseMode.ADAPTIVE: self._adaptive_response,
            ResponseMode.CONTEXTUAL: self._contextual_response,
            ResponseMode.CREATIVE: self._creative_response,
            ResponseMode.ANALYTICAL: self._analytical_response,
        }

    def _init_validation_components(self):
        """Inicializar componentes de validaci√≥n"""
        self.validation_levels = {
            ValidationLevel.BASIC: self._basic_validation,
            ValidationLevel.SEMANTIC: self._semantic_validation,
            ValidationLevel.LOGICAL: self._logical_validation,
            ValidationLevel.COMPREHENSIVE: self._comprehensive_validation,
        }

        # Inicializar modelo de embeddings para validaci√≥n sem√°ntica
        self.embedding_model = None
        if TORCH_AVAILABLE and SentenceTransformer is not None:
            try:
                self.embedding_model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")
            except Exception as e:
                logger.warning(f"No se pudo cargar modelo de embeddings: {e}")

    def _init_adaptation_components(self):
        """Inicializar componentes de adaptaci√≥n"""
        self.adaptation_strategies = {
            "prompt_enhancement": self._enhance_prompt,
            "context_integration": self._integrate_context,
            "style_adaptation": self._adapt_style,
            "complexity_adjustment": self._adjust_complexity,
        }

    async def generate_response(self, request: GenerationRequest) -> GenerationResult:
        """Generar respuesta unificada"""
        start_time = time.time()

        try:
            # Adaptar prompt si est√° habilitado
            if self.config.enable_adaptation:
                adapted_prompt = await self._adapt_prompt(request)
            else:
                adapted_prompt = request.prompt

            # Generar contenido seg√∫n tipo
            generator = self.generation_types[request.generation_type]
            raw_content = await generator(adapted_prompt, request.context)

            # Aplicar modo de respuesta
            response_processor = self.response_modes[request.response_mode]
            processed_content = await response_processor(raw_content, request.context)

            # Validar resultado
            validation_result = await self._validate_generation(processed_content, request)

            # Refinar si es necesario
            final_content = processed_content
            refinements = []
            if self.config.enable_refinement and not validation_result.is_valid:
                final_content, refinements = await self._refine_generation(
                    processed_content, validation_result
                )

            # Calcular m√©tricas
            quality_score = self._calculate_quality_score(final_content, validation_result)
            confidence = self._calculate_confidence(final_content, validation_result)

            processing_time = time.time() - start_time

            # Crear resultado
            result_id = f"gen_{int(time.time() * 1000)}"
            result = GenerationResult(
                id=result_id,
                content=final_content,
                generation_type=request.generation_type,
                response_mode=request.response_mode,
                quality_score=quality_score,
                confidence=confidence,
                processing_time=processing_time,
                created_at=datetime.now(),
                metadata=request.metadata or {},
                validation_results=validation_result.__dict__,
                refinements=refinements,
            )

            # Guardar resultado
            await self._save_generation_to_db(result)
            self.generation_history.append(result)

            return result

        except Exception as e:
            logger.error(f"Error generando respuesta: {e}")
            raise

    async def _generate_text(self, prompt: str, context: Optional[Dict[str, Any]] = None) -> str:
        """Generar texto simple"""
        try:
            # Generaci√≥n de texto b√°sica
            response = f"Respuesta: {prompt[:100]}"

            if context:
                if context.get("detailed"):
                    response += " [Respuesta detallada]"
                if context.get("domain"):
                    response += f" [Dominio: {context['domain']}]"

            return response
        except Exception as e:
            logger.error(f"Error en generaci√≥n de texto: {e}")
            return f"Texto generado para: {prompt[:50]}..."

    async def _generate_classification(
        self, prompt: str, context: Optional[Dict[str, Any]] = None
    ) -> str:
        """Generar clasificaci√≥n usando modelo especializado"""
        try:
            # Usar modelo de clasificaci√≥n real si est√° disponible
            classifier = self.components.get("classification_model")
            if classifier:
                # Clasificaci√≥n con probabilidades
                result = classifier.predict(prompt)
                return json.dumps(
                    {"category": result.category, "probabilities": result.probabilities}
                )
            else:
                # Respuesta simulada cuando no hay modelo
                categories = ["positivo", "negativo", "neutral"]
                category = categories[len(prompt) % len(categories)]
                return json.dumps(
                    {
                        "category": category,
                        "probabilities": {cat: 0.33 for cat in categories},
                        "note": "Clasificaci√≥n simulada - modelo no disponible",
                    }
                )

        except Exception as e:
            logger.error(f"Error en clasificaci√≥n: {e}")
            return json.dumps({"category": "unknown", "error": str(e)})

    async def _generate_regression(
        self, prompt: str, context: Optional[Dict[str, Any]] = None
    ) -> str:
        """Generar regresi√≥n usando modelo especializado"""
        try:
            # Usar modelo de regresi√≥n real si est√° disponible
            regressor = self.components.get("regression_model")
            if regressor:
                # Predicci√≥n con intervalo de confianza
                prediction = regressor.predict(prompt)
                return json.dumps(
                    {
                        "predicted_value": prediction.value,
                        "confidence_interval": prediction.confidence_interval,
                        "standard_error": prediction.standard_error,
                    }
                )
            else:
                # Respuesta simulada cuando no hay modelo
                simulated_value = len(prompt) * 0.5
                return json.dumps(
                    {
                        "predicted_value": simulated_value,
                        "confidence_interval": [simulated_value - 1.0, simulated_value + 1.0],
                        "standard_error": 0.5,
                        "note": "Regresi√≥n simulada - modelo no disponible",
                    }
                )

        except Exception as e:
            logger.error(f"Error en regresi√≥n: {e}")
            return json.dumps({"predicted_value": 0.0, "error": str(e)})

    async def _generate_code(self, prompt: str, context: Optional[Dict[str, Any]] = None) -> str:
        """Generar c√≥digo"""
        # Simular generaci√≥n de c√≥digo
        language = context.get("language", "python") if context else "python"
        return f"# C√≥digo {language}\ndef example():\n    return '{prompt[:20]}...'"

    async def _generate_creative(
        self, prompt: str, context: Optional[Dict[str, Any]] = None
    ) -> str:
        """Generar contenido creativo"""
        creative_elements = ["üí°", "üé®", "‚ú®", "üåü"]
        element = creative_elements[int(time.time()) % len(creative_elements)]

        return f"{element} Idea creativa: {prompt[:40]}... {element}"

    async def _generate_technical(
        self, prompt: str, context: Optional[Dict[str, Any]] = None
    ) -> str:
        """Generar contenido t√©cnico"""
        technical_terms = ["algoritmo", "optimizaci√≥n", "eficiencia", "rendimiento"]
        term = technical_terms[int(time.time()) % len(technical_terms)]

        return f"An√°lisis t√©cnico ({term}): {prompt[:35]}..."

    async def _direct_response(self, content: str, context: Optional[Dict[str, Any]] = None) -> str:
        """Respuesta directa"""
        return content

    async def _adaptive_response(
        self, content: str, context: Optional[Dict[str, Any]] = None
    ) -> str:
        """Respuesta adaptativa"""
        # Adaptar basado en contexto
        if context and context.get("user_level") == "expert":
            content += " [Modo experto]"
        elif context and context.get("user_level") == "beginner":
            content += " [Explicaci√≥n simplificada]"

        return content

    async def _contextual_response(
        self, content: str, context: Optional[Dict[str, Any]] = None
    ) -> str:
        """Respuesta contextual"""
        if context and context.get("domain"):
            content += f" [Dominio: {context['domain']}]"

        return content

    async def _creative_response(
        self, content: str, context: Optional[Dict[str, Any]] = None
    ) -> str:
        """Respuesta creativa"""
        creative_prefixes = ["üí≠", "üéØ", "üöÄ", "üí°"]
        prefix = creative_prefixes[int(time.time()) % len(creative_prefixes)]

        return f"{prefix} {content}"

    async def _analytical_response(
        self, content: str, context: Optional[Dict[str, Any]] = None
    ) -> str:
        """Respuesta anal√≠tica"""
        return f"üìä An√°lisis: {content} [Detalles t√©cnicos incluidos]"

    async def _adapt_prompt(self, request: GenerationRequest) -> str:
        """Adaptar prompt"""
        adapted_prompt = request.prompt

        # Aplicar estrategias de adaptaci√≥n
        for strategy_name, strategy_func in self.adaptation_strategies.items():
            adapted_prompt = await strategy_func(adapted_prompt, request.context)

        return adapted_prompt

    async def _enhance_prompt(self, prompt: str, context: Optional[Dict[str, Any]] = None) -> str:
        """Mejorar prompt"""
        enhancements = []

        if context and context.get("style"):
            enhancements.append(f"Estilo: {context['style']}")

        if context and context.get("tone"):
            enhancements.append(f"Tono: {context['tone']}")

        if enhancements:
            prompt += f" [{' | '.join(enhancements)}]"

        return prompt

    async def _integrate_context(
        self, prompt: str, context: Optional[Dict[str, Any]] = None
    ) -> str:
        """Integrar contexto"""
        if context and context.get("background"):
            prompt = f"Contexto: {context['background']}\n\n{prompt}"

        return prompt

    async def _adapt_style(self, prompt: str, context: Optional[Dict[str, Any]] = None) -> str:
        """Adaptar estilo"""
        if context and context.get("formal"):
            prompt += " [Formal]"
        elif context and context.get("casual"):
            prompt += " [Casual]"

        return prompt

    async def _adjust_complexity(
        self, prompt: str, context: Optional[Dict[str, Any]] = None
    ) -> str:
        """Ajustar complejidad"""
        if context and context.get("simple"):
            prompt += " [Explicaci√≥n simple]"
        elif context and context.get("detailed"):
            prompt += " [Explicaci√≥n detallada]"

        return prompt

    async def _validate_generation(
        self, content: str, request: GenerationRequest
    ) -> ValidationResult:
        """Validar generaci√≥n"""
        validator = self.validation_levels[self.config.validation_level]
        return await validator(content, request)

    async def _basic_validation(self, content: str, request: GenerationRequest) -> ValidationResult:
        """Validaci√≥n b√°sica"""
        issues = []
        suggestions = []

        # Verificar longitud
        if len(content) < 10:
            issues.append("Contenido muy corto")
            suggestions.append("Expandir la respuesta")

        # Verificar contenido vac√≠o
        if not content.strip():
            issues.append("Contenido vac√≠o")
            suggestions.append("Generar contenido v√°lido")

        is_valid = len(issues) == 0
        score = 1.0 if is_valid else 0.3

        return ValidationResult(
            is_valid=is_valid,
            score=score,
            issues=issues,
            suggestions=suggestions,
            confidence=0.8,
            validation_level=ValidationLevel.BASIC,
        )

    async def _semantic_validation(
        self, content: str, request: GenerationRequest
    ) -> ValidationResult:
        """Validaci√≥n sem√°ntica"""
        basic_result = await self._basic_validation(content, request)

        if not basic_result.is_valid:
            return basic_result

        issues = basic_result.issues.copy()
        suggestions = basic_result.suggestions.copy()

        # Verificar coherencia sem√°ntica
        if self.embedding_model:
            try:
                # Generar embedding del prompt y contenido
                prompt_embedding = self.embedding_model.encode(request.prompt)
                content_embedding = self.embedding_model.encode(content)

                # Calcular similitud
                similarity = np.dot(prompt_embedding, content_embedding) / (
                    np.linalg.norm(prompt_embedding) * np.linalg.norm(content_embedding)
                )

                if similarity < 0.3:
                    issues.append("Baja relevancia sem√°ntica")
                    suggestions.append("Mejorar la relevancia del contenido")

            except Exception as e:
                logger.warning(f"Error en validaci√≥n sem√°ntica: {e}")

        # Verificar estructura
        if request.generation_type == GenerationType.CLASSIFICATION:
            if "clasificaci√≥n" not in content.lower():
                issues.append("Falta indicador de clasificaci√≥n")
                suggestions.append("Incluir etiqueta de clasificaci√≥n")

        is_valid = len(issues) == 0
        score = max(0.5, 1.0 - len(issues) * 0.2)

        return ValidationResult(
            is_valid=is_valid,
            score=score,
            issues=issues,
            suggestions=suggestions,
            confidence=0.9,
            validation_level=ValidationLevel.SEMANTIC,
        )

    async def _logical_validation(
        self, content: str, request: GenerationRequest
    ) -> ValidationResult:
        """Validaci√≥n l√≥gica"""
        semantic_result = await self._semantic_validation(content, request)

        if not semantic_result.is_valid:
            return semantic_result

        issues = semantic_result.issues.copy()
        suggestions = semantic_result.suggestions.copy()

        # Verificar coherencia l√≥gica
        if "contradicci√≥n" in content.lower() or "contradictorio" in content.lower():
            issues.append("Contenido contradictorio")
            suggestions.append("Revisar coherencia l√≥gica")

        # Verificar consistencia
        if content.count("s√≠") > 0 and content.count("no") > 0:
            if abs(content.count("s√≠") - content.count("no")) < 2:
                issues.append("Posible inconsistencia")
                suggestions.append("Verificar consistencia de afirmaciones")

        is_valid = len(issues) == 0
        score = max(0.6, semantic_result.score - len(issues) * 0.1)

        return ValidationResult(
            is_valid=is_valid,
            score=score,
            issues=issues,
            suggestions=suggestions,
            confidence=0.95,
            validation_level=ValidationLevel.LOGICAL,
        )

    async def _comprehensive_validation(
        self, content: str, request: GenerationRequest
    ) -> ValidationResult:
        """Validaci√≥n comprehensiva"""
        logical_result = await self._logical_validation(content, request)

        if not logical_result.is_valid:
            return logical_result

        issues = logical_result.issues.copy()
        suggestions = logical_result.suggestions.copy()

        # Verificaciones adicionales
        if request.generation_type == GenerationType.CODE:
            if "def " in content or "function " in content:
                if "return" not in content:
                    issues.append("Funci√≥n sin return")
                    suggestions.append("Agregar statement de return")

        # Verificar calidad general
        word_count = len(content.split())
        if word_count < 5:
            issues.append("Contenido insuficiente")
            suggestions.append("Expandir la respuesta")

        is_valid = len(issues) == 0
        score = max(0.7, logical_result.score - len(issues) * 0.05)

        return ValidationResult(
            is_valid=is_valid,
            score=score,
            issues=issues,
            suggestions=suggestions,
            confidence=0.98,
            validation_level=ValidationLevel.COMPREHENSIVE,
        )

    async def _refine_generation(
        self, content: str, validation_result: ValidationResult
    ) -> Tuple[str, List[str]]:
        """Refinar generaci√≥n"""
        refined_content = content
        refinements = []

        # Aplicar sugerencias de validaci√≥n
        for suggestion in validation_result.suggestions:
            if "expandir" in suggestion.lower():
                refined_content += " [Contenido expandido]"
                refinements.append("Expansi√≥n de contenido")
            elif "mejorar" in suggestion.lower():
                refined_content += " [Mejorado]"
                refinements.append("Mejora de calidad")
            elif "incluir" in suggestion.lower():
                refined_content += " [Informaci√≥n adicional incluida]"
                refinements.append("Inclusi√≥n de informaci√≥n")

        return refined_content, refinements

    def _calculate_quality_score(self, content: str, validation_result: ValidationResult) -> float:
        """Calcular score de calidad"""
        base_score = validation_result.score

        # Factores adicionales
        length_factor = min(len(content) / 100, 1.0) * 0.2
        complexity_factor = min(len(content.split()) / 20, 1.0) * 0.1

        quality_score = base_score + length_factor + complexity_factor
        return min(quality_score, 1.0)

    def _calculate_confidence(self, content: str, validation_result: ValidationResult) -> float:
        """Calcular confianza"""
        base_confidence = validation_result.confidence

        # Ajustar basado en issues
        issue_penalty = len(validation_result.issues) * 0.05
        confidence = max(0.1, base_confidence - issue_penalty)

        return confidence

    async def _save_generation_to_db(self, result: GenerationResult):
        """Guardar generaci√≥n en base de datos"""
        try:
            cursor = self.conn.cursor()

            cursor.execute(
                """
                INSERT INTO generations 
                (id, prompt, content, generation_type, response_mode, quality_score, confidence,
                 processing_time, created_at, metadata, validation_results)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """,
                (
                    result.id,
                    # TODO: Store the real prompt here. No placeholders allowed.
                    result.content,
                    result.generation_type.value,
                    result.response_mode.value,
                    result.quality_score,
                    result.confidence,
                    result.processing_time,
                    result.created_at.isoformat(),
                    json.dumps(result.metadata),
                    json.dumps(result.validation_results),
                ),
            )

            self.conn.commit()
            cursor.close()

        except Exception as e:
            logger.error(f"Error guardando generaci√≥n: {e}")

    def get_system_stats(self) -> Dict[str, Any]:
        """Obtener estad√≠sticas del sistema"""
        try:
            cursor = self.conn.cursor()

            # Estad√≠sticas de generaciones
            cursor.execute("SELECT COUNT(*) FROM generations")
            total_generations = cursor.fetchone()[0]

            cursor.execute("SELECT AVG(quality_score) FROM generations")
            avg_quality = cursor.fetchone()[0] or 0.0

            cursor.execute("SELECT AVG(confidence) FROM generations")
            avg_confidence = cursor.fetchone()[0] or 0.0

            # Estad√≠sticas de validaciones
            cursor.execute("SELECT COUNT(*) FROM validations")
            total_validations = cursor.fetchone()[0]

            cursor.execute("SELECT COUNT(*) FROM validations WHERE is_valid = 1")
            valid_count = cursor.fetchone()[0]

            # Estad√≠sticas de refinamientos
            cursor.execute("SELECT COUNT(*) FROM refinements")
            total_refinements = cursor.fetchone()[0]

            cursor.close()

            return {
                "generations": {
                    "total": total_generations,
                    "average_quality": round(avg_quality, 3),
                    "average_confidence": round(avg_confidence, 3),
                },
                "validations": {
                    "total": total_validations,
                    "valid_count": valid_count,
                    "success_rate": round(valid_count / max(total_validations, 1), 3),
                },
                "refinements": {"total": total_refinements},
                "performance": {
                    "generation_types": list(
                        set(gen.generation_type.value for gen in self.generation_history)
                    ),
                    "response_modes": list(
                        set(gen.response_mode.value for gen in self.generation_history)
                    ),
                },
            }

        except Exception as e:
            logger.error(f"Error obteniendo estad√≠sticas: {e}")
            return {"error": str(e)}

    def close(self):
        """Cerrar sistema"""
        try:
            if hasattr(self, "conn"):
                self.conn.close()
            logger.info("‚úÖ Sistema de generaci√≥n y respuesta cerrado")
        except Exception as e:
            logger.error(f"Error cerrando sistema: {e}")


def get_unified_generation_response_system(
    config: Optional[GenerationConfig] = None, db_path: Optional[str] = None
) -> UnifiedGenerationResponseSystem:
    """Funci√≥n factory para crear sistema unificado"""
    return UnifiedGenerationResponseSystem(config, db_path)


async def main():
    """Funci√≥n principal de demostraci√≥n"""
    # Configurar sistema
    config = GenerationConfig(
        generation_type=GenerationType.TEXT,
        response_mode=ResponseMode.ADAPTIVE,
        validation_level=ValidationLevel.SEMANTIC,
        enable_adaptation=True,
        enable_refinement=True,
    )

    system = get_unified_generation_response_system(config)

    print("üöÄ Sistema Unificado de Generaci√≥n y Respuesta")
    print("=" * 50)

    # Ejemplo de generaci√≥n de texto
    print("\nüìù Generaci√≥n de Texto:")
    text_request = GenerationRequest(
        prompt="Explica qu√© es la inteligencia artificial",
        context={"user_level": "beginner", "domain": "technology"},
        generation_type=GenerationType.TEXT,
        response_mode=ResponseMode.ADAPTIVE,
    )

    text_result = await system.generate_response(text_request)
    print(f"   Contenido: {text_result.content}")
    print(f"   Calidad: {text_result.quality_score:.3f}")
    print(f"   Confianza: {text_result.confidence:.3f}")
    print(f"   Tiempo: {text_result.processing_time:.3f}s")

    # Ejemplo de clasificaci√≥n
    print("\nüè∑Ô∏è Generaci√≥n de Clasificaci√≥n:")
    classification_request = GenerationRequest(
        prompt="Este producto es excelente",
        generation_type=GenerationType.CLASSIFICATION,
        response_mode=ResponseMode.DIRECT,
    )

    classification_result = await system.generate_response(classification_request)
    print(f"   Resultado: {classification_result.content}")
    if classification_result.validation_results:
        print(f"   V√°lido: {classification_result.validation_results.get('is_valid', 'N/A')}")
    else:
        print(f"   V√°lido: N/A")

    # Ejemplo de generaci√≥n creativa
    print("\nüé® Generaci√≥n Creativa:")
    creative_request = GenerationRequest(
        prompt="Crea una historia corta sobre robots",
        context={"style": "imaginativo", "tone": "amigable"},
        generation_type=GenerationType.CREATIVE,
        response_mode=ResponseMode.CREATIVE,
    )

    creative_result = await system.generate_response(creative_request)
    print(f"   Contenido: {creative_result.content}")
    print(f"   Refinamientos: {len(creative_result.refinements)}")

    # Estad√≠sticas
    print("\nüìä Estad√≠sticas del Sistema:")
    stats = system.get_system_stats()
    print(f"   Generaciones totales: {stats['generations']['total']}")
    print(f"   Calidad promedio: {stats['generations']['average_quality']}")
    print(f"   Tasa de √©xito validaci√≥n: {stats['validations']['success_rate']}")
    print(f"   Refinamientos totales: {stats['refinements']['total']}")

    # Cerrar sistema
    system.close()


if __name__ == "__main__":
    asyncio.run(main())
