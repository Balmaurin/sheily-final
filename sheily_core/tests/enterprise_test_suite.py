#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SISTEMA DE TESTING EMPRESARIAL OPTIMIZADO - SHEILY AI
====================================================

Suite de testing empresarial avanzada con:
- Ejecuci√≥n paralela de tests
- M√©tricas empresariales avanzadas
- Reportes ejecutivos profesionales
- Quality gates empresariales
- An√°lisis de rendimiento optimizado
- Cobertura de c√≥digo empresarial
- Monitoreo de recursos en tiempo real
"""

import argparse
import asyncio
import json
import logging
import multiprocessing
import os
import subprocess
import sys
import time
import xml.etree.ElementTree as ET
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import psutil


@dataclass
class EnterpriseTestResult:
    """Resultado empresarial de un test individual"""

    test_name: str
    success: bool
    duration: float
    cpu_usage: float
    memory_usage: float
    message: str
    category: str
    priority: str
    details: Dict[str, Any] = None


@dataclass
class EnterpriseTestMetrics:
    """M√©tricas empresariales avanzadas"""

    total_execution_time: float = 0.0
    total_cpu_usage: float = 0.0
    total_memory_usage: float = 0.0
    tests_passed: int = 0
    tests_failed: int = 0
    critical_tests_failed: int = 0
    high_priority_tests_failed: int = 0
    average_test_duration: float = 0.0
    peak_memory_usage: float = 0.0
    peak_cpu_usage: float = 0.0
    parallel_execution_efficiency: float = 0.0
    code_coverage_percentage: float = 0.0
    enterprise_quality_score: float = 0.0


class EnterpriseTestSuite:
    """Suite de testing empresarial optimizada"""

    def __init__(self, project_root: str = "."):
        self.project_root = Path(project_root)
        self.results: List[EnterpriseTestResult] = []
        self.metrics = EnterpriseTestMetrics()
        self.start_time = time.time()
        self.process = psutil.Process()

        # Configuraci√≥n empresarial
        self.enterprise_config = {
            "parallel_workers": min(multiprocessing.cpu_count(), 8),
            "timeout_per_test": 300,
            "memory_limit_mb": 2048,
            "cpu_limit_percent": 80,
            "quality_gate_threshold": 95.0,
            "critical_test_categories": ["security", "integration", "performance"],
            "high_priority_tests": ["authentication", "data_integrity", "api_endpoints"],
        }

        # Configurar logging empresarial
        self._setup_enterprise_logging()

    def _setup_enterprise_logging(self):
        """Configurar logging empresarial profesional"""
        log_format = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
        logging.basicConfig(
            level=logging.INFO,
            format=log_format,
            handlers=[
                logging.FileHandler(self.project_root / "logs" / "enterprise_testing.log"),
                logging.StreamHandler(sys.stdout),
            ],
        )
        self.logger = logging.getLogger("EnterpriseTestSuite")

    def _monitor_system_resources(self) -> Tuple[float, float]:
        """Monitorear recursos del sistema en tiempo real"""
        try:
            cpu_percent = self.process.cpu_percent()
            memory_info = self.process.memory_info()
            memory_mb = memory_info.rss / 1024 / 1024
            return cpu_percent, memory_mb
        except Exception:
            return 0.0, 0.0

    def _run_test_with_monitoring(
        self, test_func, test_name: str, category: str, priority: str
    ) -> EnterpriseTestResult:
        """Ejecutar test individual con monitoreo empresarial"""
        test_start = time.time()
        cpu_start, memory_start = self._monitor_system_resources()

        try:
            # Ejecutar test con timeout empresarial
            result = test_func()
            success = result.get("success", False)
            message = result.get("message", "Test ejecutado")

        except subprocess.TimeoutExpired:
            success = False
            message = f"Test timeout despu√©s de {self.enterprise_config['timeout_per_test']}s"
        except Exception as e:
            success = False
            message = f"Error ejecutando test: {str(e)}"

        # Monitoreo final de recursos
        test_end = time.time()
        cpu_end, memory_end = self._monitor_system_resources()

        # Calcular m√©tricas del test
        duration = test_end - test_start
        avg_cpu = (cpu_start + cpu_end) / 2
        avg_memory = (memory_start + memory_end) / 2

        return EnterpriseTestResult(
            test_name=test_name,
            success=success,
            duration=duration,
            cpu_usage=avg_cpu,
            memory_usage=avg_memory,
            message=message,
            category=category,
            priority=priority,
            details={
                "cpu_start": cpu_start,
                "cpu_end": cpu_end,
                "memory_start": memory_start,
                "memory_end": memory_end,
                "test_output": result if isinstance(result, dict) else {},
            },
        )

    def run_unit_tests_enterprise(self) -> List[EnterpriseTestResult]:
        """Ejecutar tests unitarios empresariales optimizados"""
        self.logger.info("üöÄ Ejecutando tests unitarios empresariales...")

        unit_tests = [
            ("test_chat_engine_real", "chat", "critical"),
            ("test_branch_detection_real", "branch_detection", "high"),
            ("test_model_interface_real", "model_interface", "high"),
            ("test_configuration_real", "config", "critical"),
            ("test_health_check_real", "health", "medium"),
            ("test_context_management_real", "context", "medium"),
        ]

        results = []
        for test_name, category, priority in unit_tests:
            result = self._run_test_with_monitoring(
                lambda: self._execute_real_functionality_test(test_name),
                test_name,
                category,
                priority,
            )
            results.append(result)

        return results

    def run_integration_tests_enterprise(self) -> List[EnterpriseTestResult]:
        """Ejecutar tests de integraci√≥n empresariales"""
        self.logger.info("üîó Ejecutando tests de integraci√≥n empresariales...")

        integration_tests = [
            ("test_full_system", "integration", "critical"),
            ("test_api_endpoints", "api", "critical"),
            ("test_database_integration", "database", "high"),
            ("test_external_services", "external", "medium"),
        ]

        results = []
        for test_name, category, priority in integration_tests:
            result = self._run_test_with_monitoring(
                lambda: self._execute_pytest_test(f"sheily_core/tests/test_{test_name}.py"),
                test_name,
                category,
                priority,
            )
            results.append(result)

        return results

    def run_security_tests_enterprise(self) -> List[EnterpriseTestResult]:
        """Ejecutar tests de seguridad empresariales"""
        self.logger.info("üîí Ejecutando tests de seguridad empresariales...")

        security_tests = [
            ("test_authentication", "security", "critical"),
            ("test_authorization", "security", "critical"),
            ("test_data_encryption", "security", "critical"),
            ("test_input_validation", "security", "high"),
            ("test_vulnerability_scanning", "security", "high"),
        ]

        results = []
        for test_name, category, priority in security_tests:
            result = self._run_test_with_monitoring(
                lambda: self._execute_pytest_test(f"sheily_core/tests/test_{test_name}.py"),
                test_name,
                category,
                priority,
            )
            results.append(result)

        return results

    def run_performance_tests_enterprise(self) -> List[EnterpriseTestResult]:
        """Ejecutar tests de rendimiento empresariales"""
        self.logger.info("‚ö° Ejecutando tests de rendimiento empresariales...")

        performance_tests = [
            ("test_response_time", "performance", "high"),
            ("test_memory_usage", "performance", "high"),
            ("test_cpu_utilization", "performance", "medium"),
            ("test_scalability", "performance", "medium"),
            ("test_load_testing", "performance", "low"),
        ]

        results = []
        for test_name, category, priority in performance_tests:
            result = self._run_test_with_monitoring(
                lambda: self._execute_pytest_test(f"sheily_core/tests/test_{test_name}.py"),
                test_name,
                category,
                priority,
            )
            results.append(result)

        return results

    def _execute_pytest_test(self, test_file: str) -> Dict[str, Any]:
        """Ejecutar test pytest individual optimizado"""
        try:
            result = subprocess.run(
                [sys.executable, "-m", "pytest", test_file, "-v", "--tb=short", "--durations=0"],
                capture_output=True,
                text=True,
                timeout=self.enterprise_config["timeout_per_test"],
            )

            return {
                "success": result.returncode == 0,
                "return_code": result.returncode,
                "stdout": result.stdout,
                "stderr": result.stderr,
            }
        except subprocess.TimeoutExpired:
            return {
                "success": False,
                "error": "Timeout",
                "message": f'Test excedi√≥ timeout de {self.enterprise_config["timeout_per_test"]}s',
            }
        except FileNotFoundError:
            return {
                "success": False,
                "error": "FileNotFound",
                "message": f"Archivo de test no encontrado: {test_file}",
            }

    def _execute_real_functionality_test(self, test_name: str) -> Dict[str, Any]:
        """Ejecutar tests reales de funcionalidades existentes"""
        try:
            # Importar funcionalidades reales de Sheily AI
            sys.path.append(str(self.project_root))

            if test_name == "test_chat_engine_real":
                return self._test_chat_engine_real()
            elif test_name == "test_branch_detection_real":
                return self._test_branch_detection_real()
            elif test_name == "test_model_interface_real":
                return self._test_model_interface_real()
            elif test_name == "test_configuration_real":
                return self._test_configuration_real()
            elif test_name == "test_health_check_real":
                return self._test_health_check_real()
            elif test_name == "test_context_management_real":
                return self._test_context_management_real()
            else:
                return {
                    "success": False,
                    "error": "UnknownTest",
                    "message": f"Test no reconocido: {test_name}",
                }

        except Exception as e:
            return {
                "success": False,
                "error": "Exception",
                "message": f"Error ejecutando test real: {str(e)}",
            }

    def _test_chat_engine_real(self) -> Dict[str, Any]:
        """Test real del motor de chat de Sheily AI"""
        try:
            from sheily_core.chat_engine import ChatMessage, ChatResponse, create_chat_engine

            # Crear motor de chat real
            chat_engine = create_chat_engine()

            # Probar consulta b√°sica
            query = "¬øQu√© es la inteligencia artificial?"
            response = chat_engine(query, "test_client")

            # Verificar respuesta real
            if not isinstance(response, ChatResponse):
                return {
                    "success": False,
                    "message": f"Tipo de respuesta incorrecto: {type(response)}",
                }

            if not response.response or len(response.response) < 10:
                return {
                    "success": False,
                    "message": f"Respuesta muy corta: {len(response.response)} caracteres",
                }

            if response.branch not in [
                "general",
                "inteligencia artificial",
                "programaci√≥n",
                "medicina",
            ]:
                return {"success": False, "message": f"Rama detectada inv√°lida: {response.branch}"}

            return {
                "success": True,
                "message": "Motor de chat funcional",
                "details": {
                    "response_length": len(response.response),
                    "branch_detected": response.branch,
                    "confidence": response.confidence,
                    "processing_time": response.processing_time,
                },
            }

        except ImportError as e:
            return {
                "success": False,
                "error": "ImportError",
                "message": f"No se pudo importar chat_engine: {e}",
            }
        except Exception as e:
            return {
                "success": False,
                "error": "Exception",
                "message": f"Error en test de chat: {e}",
            }

    def _test_branch_detection_real(self) -> Dict[str, Any]:
        """Test real de detecci√≥n de ramas acad√©micas"""
        try:
            from sheily_core.chat_engine import detect_branch, get_default_branches

            # Obtener configuraci√≥n de ramas real
            branches_config = get_default_branches()

            if not branches_config:
                return {"success": False, "message": "No se pudieron cargar ramas acad√©micas"}

            # Probar diferentes consultas acad√©micas
            test_queries = [
                ("¬øQu√© es una funci√≥n recursiva?", "programaci√≥n"),
                ("¬øC√≥mo funciona el coraz√≥n?", "medicina"),
                ("¬øQu√© es machine learning?", "inteligencia artificial"),
                ("¬øCu√°l es la capital de Espa√±a?", "general"),
            ]

            successful_detections = 0
            for query, expected_branch in test_queries:
                detected_branch, confidence = detect_branch(query, branches_config)

                if detected_branch in branches_config:
                    successful_detections += 1

            success_rate = successful_detections / len(test_queries)

            if success_rate >= 0.75:  # Al menos 75% de precisi√≥n
                return {
                    "success": True,
                    "message": f"Detecci√≥n de ramas funcional ({success_rate:.1%} precisi√≥n)",
                    "details": {
                        "success_rate": success_rate,
                        "total_queries": len(test_queries),
                        "successful_detections": successful_detections,
                        "branches_available": list(branches_config.keys()),
                    },
                }
            else:
                return {"success": False, "message": f"Precisi√≥n baja: {success_rate:.1%} < 75%"}

        except Exception as e:
            return {
                "success": False,
                "error": "Exception",
                "message": f"Error en test de detecci√≥n de ramas: {e}",
            }

    def _test_model_interface_real(self) -> Dict[str, Any]:
        """Test real de interfaz de modelo"""
        try:
            from sheily_core.chat_engine import create_model_interface, validate_model_files

            # Verificar si hay modelo disponible
            config_file = self.project_root / "sheily_core" / "config.py"
            if not config_file.exists():
                return {"success": False, "message": "Archivo de configuraci√≥n no encontrado"}

            # Intentar crear interfaz de modelo (sin modelo real)
            try:
                # Esto deber√≠a fallar graciosamente si no hay modelo
                model_interface = create_model_interface(
                    model_path="/nonexistent/model.gguf",
                    llama_binary_path="/nonexistent/llama-cli",
                    config=None,
                )
                return {"success": False, "message": "Se cre√≥ interfaz con paths inexistentes"}
            except FileNotFoundError:
                # Comportamiento esperado cuando no hay modelo
                return {
                    "success": True,
                    "message": "Interfaz de modelo maneja archivos inexistentes correctamente",
                    "details": {"model_configured": False, "fallback_behavior": "correct"},
                }
            except Exception as e:
                return {"success": False, "message": f"Error inesperado: {e}"}

        except Exception as e:
            return {
                "success": False,
                "error": "Exception",
                "message": f"Error en test de interfaz de modelo: {e}",
            }

    def _test_configuration_real(self) -> Dict[str, Any]:
        """Test real de configuraci√≥n del sistema"""
        try:
            from sheily_core.config import get_config

            # Obtener configuraci√≥n real
            config = get_config()

            # Verificar configuraci√≥n b√°sica
            if not hasattr(config, "model_path"):
                return {"success": False, "message": "Configuraci√≥n incompleta - falta model_path"}

            if not hasattr(config, "corpus_root"):
                return {"success": False, "message": "Configuraci√≥n incompleta - falta corpus_root"}

            # Verificar paths de configuraci√≥n
            corpus_path = Path(config.corpus_root)
            if corpus_path.exists():
                return {
                    "success": True,
                    "message": "Configuraci√≥n v√°lida y corpus accesible",
                    "details": {
                        "corpus_exists": True,
                        "corpus_path": str(corpus_path),
                        "model_configured": bool(config.model_path),
                    },
                }
            else:
                return {
                    "success": True,
                    "message": "Configuraci√≥n v√°lida pero corpus no existe",
                    "details": {
                        "corpus_exists": False,
                        "corpus_path": str(corpus_path),
                        "model_configured": bool(config.model_path),
                    },
                }

        except Exception as e:
            return {
                "success": False,
                "error": "Exception",
                "message": f"Error en test de configuraci√≥n: {e}",
            }

    def _test_health_check_real(self) -> Dict[str, Any]:
        """Test real de health checks del sistema"""
        try:
            from sheily_core.chat_engine import create_chat_context, perform_health_check

            # Crear contexto real
            context = create_chat_context()

            # Ejecutar health check real
            health_status = perform_health_check(context)

            # Verificar estructura de respuesta
            if not isinstance(health_status, dict):
                return {
                    "success": False,
                    "message": f"Tipo de respuesta incorrecto: {type(health_status)}",
                }

            required_keys = ["status", "components", "timestamp"]
            for key in required_keys:
                if key not in health_status:
                    return {"success": False, "message": f"Clave faltante en health check: {key}"}

            # Verificar componentes cr√≠ticos
            components = health_status["components"]
            critical_components = ["branch_detector", "context_manager"]

            for component in critical_components:
                if component not in components:
                    return {
                        "success": False,
                        "message": f"Componente cr√≠tico faltante: {component}",
                    }

            return {
                "success": True,
                "message": f'Health check funcional - Estado: {health_status["status"]}',
                "details": {
                    "overall_status": health_status["status"],
                    "components_checked": len(components),
                    "timestamp": health_status["timestamp"],
                },
            }

        except Exception as e:
            return {
                "success": False,
                "error": "Exception",
                "message": f"Error en test de health check: {e}",
            }

    def _test_context_management_real(self) -> Dict[str, Any]:
        """Test real de gesti√≥n de contexto"""
        try:
            from sheily_core.chat_engine import create_context_manager, get_context_for_query

            # Crear gestor de contexto real
            context_manager = create_context_manager("data")

            # Probar consulta de contexto
            query = "¬øQu√© es Python?"
            branch = "programaci√≥n"
            context_docs = context_manager(query, branch, max_docs=3)

            # Verificar respuesta
            if not isinstance(context_docs, list):
                return {
                    "success": False,
                    "message": f"Tipo de respuesta incorrecto: {type(context_docs)}",
                }

            # Verificar que hay contexto (aunque sea b√°sico)
            if len(context_docs) == 0:
                return {"success": False, "message": "No se gener√≥ contexto para la consulta"}

            # Verificar longitud razonable del contexto
            total_context_length = sum(len(doc) for doc in context_docs)
            if total_context_length < 50:
                return {
                    "success": False,
                    "message": f"Contexto muy corto: {total_context_length} caracteres",
                }

            return {
                "success": True,
                "message": "Gesti√≥n de contexto funcional",
                "details": {
                    "context_docs_count": len(context_docs),
                    "total_context_length": total_context_length,
                    "branch_processed": branch,
                    "query_processed": query[:50] + "...",
                },
            }

        except Exception as e:
            return {
                "success": False,
                "error": "Exception",
                "message": f"Error en test de gesti√≥n de contexto: {e}",
            }

    def run_enterprise_test_suite(self) -> Dict[str, Any]:
        """Ejecutar suite completa de testing empresarial"""
        self.logger.info("üè¢ INICIANDO SUITE DE TESTING EMPRESARIAL COMPLETA")
        self.logger.info("=" * 70)

        # Ejecutar diferentes categor√≠as de tests
        test_categories = [
            ("unit_tests", self.run_unit_tests_enterprise),
            ("integration_tests", self.run_integration_tests_enterprise),
            ("security_tests", self.run_security_tests_enterprise),
            ("performance_tests", self.run_performance_tests_enterprise),
        ]

        # Ejecutar tests de manera paralela para optimizaci√≥n empresarial
        with ThreadPoolExecutor(max_workers=self.enterprise_config["parallel_workers"]) as executor:
            future_to_category = {
                executor.submit(category_func): category_name
                for category_name, category_func in test_categories
            }

            for future in as_completed(future_to_category):
                category_name = future_to_category[future]
                try:
                    category_results = future.result()
                    self.results.extend(category_results)
                    self.logger.info(f"‚úÖ {category_name}: {len(category_results)} tests ejecutados")
                except Exception as e:
                    self.logger.error(f"‚ùå Error en {category_name}: {e}")

        # Calcular m√©tricas empresariales
        self._calculate_enterprise_metrics()

        # Generar reporte empresarial
        return self._generate_enterprise_report()

    def _calculate_enterprise_metrics(self):
        """Calcular m√©tricas empresariales avanzadas"""
        if not self.results:
            return

        # M√©tricas b√°sicas
        self.metrics.total_execution_time = time.time() - self.start_time
        self.metrics.tests_passed = sum(1 for r in self.results if r.success)
        self.metrics.tests_failed = len(self.results) - self.metrics.tests_passed

        # M√©tricas de recursos
        total_cpu = sum(r.cpu_usage for r in self.results)
        total_memory = sum(r.memory_usage for r in self.results)
        self.metrics.total_cpu_usage = total_cpu
        self.metrics.total_memory_usage = total_memory

        # M√©tricas de rendimiento
        if self.results:
            self.metrics.average_test_duration = sum(r.duration for r in self.results) / len(
                self.results
            )
            self.metrics.peak_cpu_usage = max((r.cpu_usage for r in self.results), default=0)
            self.metrics.peak_memory_usage = max((r.memory_usage for r in self.results), default=0)

        # M√©tricas cr√≠ticas
        for result in self.results:
            if not result.success:
                if result.category in self.enterprise_config["critical_test_categories"]:
                    self.metrics.critical_tests_failed += 1
                if result.priority == "critical":
                    self.metrics.high_priority_tests_failed += 1

        # Calcular puntuaci√≥n empresarial
        self._calculate_enterprise_quality_score()

    def _calculate_enterprise_quality_score(self):
        """Calcular puntuaci√≥n de calidad empresarial"""
        base_score = 100.0

        # Penalizaciones por fallos cr√≠ticos
        if self.metrics.critical_tests_failed > 0:
            base_score -= self.metrics.critical_tests_failed * 10

        if self.metrics.high_priority_tests_failed > 0:
            base_score -= self.metrics.high_priority_tests_failed * 5

        # Penalizaci√≥n por cobertura baja
        success_rate = (self.metrics.tests_passed / len(self.results)) * 100 if self.results else 0
        if success_rate < self.enterprise_config["quality_gate_threshold"]:
            base_score -= (self.enterprise_config["quality_gate_threshold"] - success_rate) * 0.5

        # Penalizaci√≥n por uso excesivo de recursos
        if self.metrics.peak_memory_usage > self.enterprise_config["memory_limit_mb"]:
            base_score -= 5

        if self.metrics.peak_cpu_usage > self.enterprise_config["cpu_limit_percent"]:
            base_score -= 5

        self.metrics.enterprise_quality_score = max(0.0, min(100.0, base_score))

    def _generate_enterprise_report(self) -> Dict[str, Any]:
        """Generar reporte empresarial completo"""
        success_rate = (self.metrics.tests_passed / len(self.results)) * 100 if self.results else 0

        # An√°lisis de tendencias empresariales
        trends = self._analyze_enterprise_trends()

        # Recomendaciones empresariales
        recommendations = self._generate_enterprise_recommendations()

        report = {
            "enterprise_summary": {
                "total_tests": len(self.results),
                "tests_passed": self.metrics.tests_passed,
                "tests_failed": self.metrics.tests_failed,
                "success_rate": success_rate,
                "enterprise_quality_score": self.metrics.enterprise_quality_score,
                "total_execution_time": self.metrics.total_execution_time,
                "average_test_duration": self.metrics.average_test_duration,
                "peak_memory_usage": self.metrics.peak_memory_usage,
                "peak_cpu_usage": self.metrics.peak_cpu_usage,
                "critical_tests_failed": self.metrics.critical_tests_failed,
                "high_priority_tests_failed": self.metrics.high_priority_tests_failed,
                "test_timestamp": time.strftime("%Y-%m-%d %H:%M:%S UTC"),
                "quality_gate_passed": self.metrics.enterprise_quality_score
                >= self.enterprise_config["quality_gate_threshold"],
            },
            "enterprise_metrics": {
                "parallel_execution_efficiency": self.metrics.parallel_execution_efficiency,
                "resource_utilization_score": self._calculate_resource_efficiency(),
                "test_distribution_by_category": self._analyze_test_distribution(),
                "performance_benchmarks": self._analyze_performance_benchmarks(),
            },
            "results": [
                {
                    "test_name": r.test_name,
                    "success": r.success,
                    "duration": r.duration,
                    "cpu_usage": r.cpu_usage,
                    "memory_usage": r.memory_usage,
                    "category": r.category,
                    "priority": r.priority,
                    "message": r.message,
                    "details": r.details,
                }
                for r in self.results
            ],
            "enterprise_analysis": {
                "trends": trends,
                "recommendations": recommendations,
                "risk_assessment": self._assess_enterprise_risks(),
                "optimization_opportunities": self._identify_optimization_opportunities(),
            },
        }

        return report

    def _analyze_enterprise_trends(self) -> Dict[str, Any]:
        """Analizar tendencias empresariales"""
        if not self.results:
            return {}

        # An√°lisis por categor√≠a
        category_analysis = {}
        for result in self.results:
            if result.category not in category_analysis:
                category_analysis[result.category] = {"passed": 0, "failed": 0, "total": 0}
            category_analysis[result.category]["total"] += 1
            if result.success:
                category_analysis[result.category]["passed"] += 1
            else:
                category_analysis[result.category]["failed"] += 1

        return {
            "category_performance": category_analysis,
            "execution_time_trend": "optimized"
            if self.metrics.average_test_duration < 30
            else "needs_optimization",
            "resource_usage_trend": "efficient"
            if self.metrics.peak_memory_usage < 1000
            else "resource_intensive",
        }

    def _generate_enterprise_recommendations(self) -> List[str]:
        """Generar recomendaciones empresariales"""
        recommendations = []

        if self.metrics.critical_tests_failed > 0:
            recommendations.append(
                "üî¥ CR√çTICO: Revisar y corregir tests cr√≠ticos fallidos inmediatamente"
            )

        if self.metrics.high_priority_tests_failed > 0:
            recommendations.append(
                "üü° ALTA PRIORIDAD: Atender tests de alta prioridad en el siguiente sprint"
            )

        if self.metrics.enterprise_quality_score < 90:
            recommendations.append(
                "üü† MEJORA: Implementar mejoras de calidad para alcanzar est√°ndares empresariales"
            )

        if self.metrics.peak_memory_usage > self.enterprise_config["memory_limit_mb"]:
            recommendations.append(
                "üîµ OPTIMIZACI√ìN: Optimizar uso de memoria en tests de rendimiento"
            )

        if self.metrics.average_test_duration > 60:
            recommendations.append(
                "‚ö° VELOCIDAD: Paralelizar tests lentos para mejorar tiempos de ejecuci√≥n"
            )

        return recommendations

    def _assess_enterprise_risks(self) -> Dict[str, Any]:
        """Evaluar riesgos empresariales"""
        risk_level = "LOW"
        risk_factors = []

        if self.metrics.critical_tests_failed > 0:
            risk_level = "CRITICAL"
            risk_factors.append("Tests cr√≠ticos fallidos representan riesgo de despliegue")

        if self.metrics.enterprise_quality_score < 85:
            if risk_level != "CRITICAL":
                risk_level = "HIGH"
            risk_factors.append("Puntuaci√≥n de calidad baja afecta confiabilidad del sistema")

        if self.metrics.peak_memory_usage > self.enterprise_config["memory_limit_mb"] * 1.5:
            risk_factors.append("Alto consumo de memoria puede afectar estabilidad en producci√≥n")

        return {
            "overall_risk_level": risk_level,
            "risk_factors": risk_factors,
            "risk_score": min(100, len(risk_factors) * 25),
        }

    def _identify_optimization_opportunities(self) -> List[str]:
        """Identificar oportunidades de optimizaci√≥n"""
        opportunities = []

        # Optimizaci√≥n de paralelizaci√≥n
        if self.metrics.parallel_execution_efficiency < 70:
            opportunities.append("Paralelizar tests secuenciales para mejorar eficiencia")

        # Optimizaci√≥n de recursos
        if self.metrics.peak_memory_usage > 1500:
            opportunities.append("Implementar limpieza de memoria entre tests")

        # Optimizaci√≥n de cobertura
        if self.metrics.tests_failed > len(self.results) * 0.2:
            opportunities.append("Mejorar cobertura de tests para reducir tasa de fallos")

        return opportunities

    def _calculate_resource_efficiency(self) -> float:
        """Calcular eficiencia de recursos empresarial"""
        if not self.results:
            return 0.0

        # Eficiencia basada en uso de recursos vs tiempo de ejecuci√≥n
        avg_memory_per_test = self.metrics.total_memory_usage / len(self.results)
        efficiency_score = 100.0

        if avg_memory_per_test > 500:  # M√°s de 500MB promedio
            efficiency_score -= 20

        if self.metrics.average_test_duration > 45:  # M√°s de 45s promedio
            efficiency_score -= 15

        if self.metrics.peak_cpu_usage > 90:  # Uso de CPU muy alto
            efficiency_score -= 10

        return max(0.0, min(100.0, efficiency_score))

    def _analyze_test_distribution(self) -> Dict[str, Any]:
        """Analizar distribuci√≥n de tests por categor√≠a"""
        distribution = {}
        for result in self.results:
            category = result.category
            if category not in distribution:
                distribution[category] = {"total": 0, "passed": 0, "failed": 0}
            distribution[category]["total"] += 1
            if result.success:
                distribution[category]["passed"] += 1
            else:
                distribution[category]["failed"] += 1

        return distribution

    def _analyze_performance_benchmarks(self) -> Dict[str, Any]:
        """Analizar benchmarks de rendimiento"""
        benchmarks = {
            "execution_time_benchmark": "EXCELLENT"
            if self.metrics.average_test_duration < 30
            else "NEEDS_IMPROVEMENT",
            "memory_usage_benchmark": "EXCELLENT"
            if self.metrics.peak_memory_usage < 1000
            else "NEEDS_IMPROVEMENT",
            "cpu_usage_benchmark": "EXCELLENT"
            if self.metrics.peak_cpu_usage < 70
            else "NEEDS_IMPROVEMENT",
            "parallel_efficiency_benchmark": "EXCELLENT"
            if self.metrics.parallel_execution_efficiency > 80
            else "NEEDS_IMPROVEMENT",
        }

        return benchmarks

    def save_enterprise_report(self, report: Dict[str, Any], format: str = "both") -> None:
        """Guardar reporte empresarial en m√∫ltiples formatos"""
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        reports_dir = self.project_root / "reports" / "enterprise"
        reports_dir.mkdir(parents=True, exist_ok=True)

        if format in ["json", "both"]:
            json_file = reports_dir / f"enterprise_test_report_{timestamp}.json"
            with open(json_file, "w", encoding="utf-8") as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            self.logger.info(f"üìä Reporte empresarial JSON guardado: {json_file}")

        if format in ["html", "both"]:
            html_file = reports_dir / f"enterprise_test_report_{timestamp}.html"
            self._save_html_report(report, html_file)
            self.logger.info(f"üåê Reporte empresarial HTML guardado: {html_file}")

        if format in ["text", "both"]:
            txt_file = reports_dir / f"enterprise_test_report_{timestamp}.txt"
            self._save_text_report(report, txt_file)
            self.logger.info(f"üìÑ Reporte empresarial texto guardado: {txt_file}")

    def _save_html_report(self, report: Dict[str, Any], file_path: Path) -> None:
        """Guardar reporte en formato HTML empresarial"""
        summary = report["enterprise_summary"]

        html_content = f"""
        <!DOCTYPE html>
        <html lang="es">
        <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>Reporte de Testing Empresarial - Sheily AI</title>
            <style>
                body {{ font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; margin: 40px; background: #f5f5f5; }}
                .header {{ background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 30px; border-radius: 10px; text-align: center; }}
                .summary {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 20px; margin: 30px 0; }}
                .metric-card {{ background: white; padding: 20px; border-radius: 8px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }}
                .metric-value {{ font-size: 2em; font-weight: bold; color: #333; }}
                .metric-label {{ color: #666; font-size: 0.9em; }}
                .success {{ color: #28a745; }}
                .warning {{ color: #ffc107; }}
                .danger {{ color: #dc3545; }}
                .section {{ background: white; margin: 20px 0; padding: 25px; border-radius: 8px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }}
                .section h2 {{ color: #333; border-bottom: 2px solid #667eea; padding-bottom: 10px; }}
                table {{ width: 100%; border-collapse: collapse; margin-top: 15px; }}
                th, td {{ padding: 12px; text-align: left; border-bottom: 1px solid #ddd; }}
                th {{ background-color: #f8f9fa; font-weight: 600; }}
            </style>
        </head>
        <body>
            <div class="header">
                <h1>üè¢ Reporte de Testing Empresarial</h1>
                <h2>Sistema Sheily AI</h2>
                <p>Generado: {summary['test_timestamp']}</p>
            </div>

            <div class="summary">
                <div class="metric-card">
                    <div class="metric-value {self._get_status_class(summary['enterprise_quality_score'])}">{summary['enterprise_quality_score']:.1f}</div>
                    <div class="metric-label">Puntuaci√≥n de Calidad Empresarial</div>
                </div>
                <div class="metric-card">
                    <div class="metric-value {self._get_status_class(summary['success_rate'])}">{summary['success_rate']:.1f}%</div>
                    <div class="metric-label">Tasa de √âxito</div>
                </div>
                <div class="metric-card">
                    <div class="metric-value">{summary['total_tests']}</div>
                    <div class="metric-label">Tests Ejecutados</div>
                </div>
                <div class="metric-card">
                    <div class="metric-value">{summary['total_execution_time']:.1f}s</div>
                    <div class="metric-label">Tiempo Total</div>
                </div>
            </div>

            <div class="section">
                <h2>üìä M√©tricas de Rendimiento</h2>
                <table>
                    <tr><th>M√©trica</th><th>Valor</th><th>Estado</th></tr>
                    <tr><td>Tiempo Promedio por Test</td><td>{summary['average_test_duration']:.2f}s</td><td>{self._get_performance_status(summary['average_test_duration'])}</td></tr>
                    <tr><td>Uso M√°ximo de Memoria</td><td>{summary['peak_memory_usage']:.0f} MB</td><td>{self._get_memory_status(summary['peak_memory_usage'])}</td></tr>
                    <tr><td>Uso M√°ximo de CPU</td><td>{summary['peak_cpu_usage']:.1f}%</td><td>{self._get_cpu_status(summary['peak_cpu_usage'])}</td></tr>
                    <tr><td>Tests Cr√≠ticos Fallidos</td><td>{summary['critical_tests_failed']}</td><td>{self._get_critical_status(summary['critical_tests_failed'])}</td></tr>
                </table>
            </div>

            <div class="section">
                <h2>üéØ Estado del Quality Gate Empresarial</h2>
                <p style="font-size: 1.2em; {'color: green;' if summary['quality_gate_passed'] else 'color: red;'}">
                    {'‚úÖ APROBADO - Cumple est√°ndares empresariales' if summary['quality_gate_passed'] else '‚ùå RECHAZADO - Requiere atenci√≥n inmediata'}
                </p>
            </div>
        </body>
        </html>
        """

        with open(file_path, "w", encoding="utf-8") as f:
            f.write(html_content)

    def _save_text_report(self, report: Dict[str, Any], file_path: Path) -> None:
        """Guardar reporte en formato texto empresarial"""
        summary = report["enterprise_summary"]

        with open(file_path, "w", encoding="utf-8") as f:
            f.write("üè¢ REPORTE DE TESTING EMPRESARIAL - SHEILY AI\n")
            f.write("=" * 70 + "\n\n")

            # Resumen ejecutivo
            f.write("üìä RESUMEN EJECUTIVO EMPRESARIAL\n")
            f.write("-" * 40 + "\n")
            f.write(f"Fecha de Generaci√≥n: {summary['test_timestamp']}\n")
            f.write(
                f"Puntuaci√≥n de Calidad Empresarial: {summary['enterprise_quality_score']:.1f}/100\n"
            )
            f.write(f"Tasa de √âxito: {summary['success_rate']:.1f}%\n")
            f.write(f"Tests Ejecutados: {summary['total_tests']}\n")
            f.write(f"Tests Aprobados: {summary['tests_passed']}\n")
            f.write(f"Tests Fallidos: {summary['tests_failed']}\n")
            f.write(f"Tiempo Total de Ejecuci√≥n: {summary['total_execution_time']:.2f}s\n")
            f.write(f"Tiempo Promedio por Test: {summary['average_test_duration']:.2f}s\n\n")

            # Estado del quality gate
            f.write("üéØ ESTADO DEL QUALITY GATE EMPRESARIAL\n")
            f.write("-" * 45 + "\n")
            if summary["quality_gate_passed"]:
                f.write("‚úÖ APROBADO - El sistema cumple con los est√°ndares empresariales\n")
            else:
                f.write("‚ùå RECHAZADO - El sistema requiere atenci√≥n inmediata\n")
            f.write(f"Tests Cr√≠ticos Fallidos: {summary['critical_tests_failed']}\n")
            f.write(
                f"Tests de Alta Prioridad Fallidos: {summary['high_priority_tests_failed']}\n\n"
            )

            # M√©tricas de recursos
            f.write("‚ö° M√âTRICAS DE RENDIMIENTO\n")
            f.write("-" * 30 + "\n")
            f.write(f"Uso M√°ximo de Memoria: {summary['peak_memory_usage']:.0f} MB\n")
            f.write(f"Uso M√°ximo de CPU: {summary['peak_cpu_usage']:.1f}%\n")
            f.write(
                f"Eficiencia de Ejecuci√≥n Paralela: {report['enterprise_metrics']['resource_utilization_score']:.1f}%\n\n"
            )

    def _get_status_class(self, value: float) -> str:
        """Obtener clase CSS basada en valor"""
        if value >= 90:
            return "success"
        elif value >= 70:
            return "warning"
        else:
            return "danger"

    def _get_performance_status(self, duration: float) -> str:
        """Obtener estado de rendimiento basado en duraci√≥n"""
        if duration < 30:
            return "üü¢ Excelente"
        elif duration < 60:
            return "üü° Bueno"
        else:
            return "üî¥ Necesita Optimizaci√≥n"

    def _get_memory_status(self, memory: float) -> str:
        """Obtener estado de memoria basado en uso"""
        if memory < 1000:
            return "üü¢ Eficiente"
        elif memory < 1500:
            return "üü° Moderado"
        else:
            return "üî¥ Alto Consumo"

    def _get_cpu_status(self, cpu: float) -> str:
        """Obtener estado de CPU basado en uso"""
        if cpu < 70:
            return "üü¢ Eficiente"
        elif cpu < 90:
            return "üü° Alto Uso"
        else:
            return "üî¥ Muy Alto"

    def _get_critical_status(self, failed: int) -> str:
        """Obtener estado basado en tests cr√≠ticos fallidos"""
        if failed == 0:
            return "üü¢ Sin Fallos"
        elif failed <= 2:
            return "üü° Algunos Fallos"
        else:
            return "üî¥ Muchos Fallos"


def main():
    """Funci√≥n principal del sistema de testing empresarial"""
    parser = argparse.ArgumentParser(
        description="Sistema de Testing Empresarial Optimizado - Sheily AI"
    )
    parser.add_argument(
        "mode",
        choices=["unit", "integration", "security", "performance", "all"],
        help="Modo de testing empresarial",
    )
    parser.add_argument(
        "--format",
        "-f",
        choices=["json", "html", "text", "both"],
        default="both",
        help="Formato del reporte empresarial",
    )
    parser.add_argument("--verbose", "-v", action="store_true", help="Salida detallada empresarial")

    args = parser.parse_args()

    # Crear suite de testing empresarial
    suite = EnterpriseTestSuite()

    print("üè¢ SISTEMA DE TESTING EMPRESARIAL OPTIMIZADO - SHEILY AI")
    print("=" * 70)
    print(f"Modo: {args.mode.upper()}")
    print(f"Formato de reporte: {args.format}")
    print(f"Workers paralelos: {suite.enterprise_config['parallel_workers']}")
    print("=" * 70)

    # Ejecutar tests seg√∫n modo seleccionado
    if args.mode == "unit":
        suite.results.extend(suite.run_unit_tests_enterprise())
    elif args.mode == "integration":
        suite.results.extend(suite.run_integration_tests_enterprise())
    elif args.mode == "security":
        suite.results.extend(suite.run_security_tests_enterprise())
    elif args.mode == "performance":
        suite.results.extend(suite.run_performance_tests_enterprise())
    elif args.mode == "all":
        report = suite.run_enterprise_test_suite()
        suite.save_enterprise_report(report, args.format)

        # Mostrar resumen ejecutivo
        summary = report["enterprise_summary"]
        print("\nüìä RESUMEN EJECUTIVO EMPRESARIAL:")
        print(f"   Puntuaci√≥n de Calidad: {summary['enterprise_quality_score']:.1f}/100")
        print(f"   Tasa de √âxito: {summary['success_rate']:.1f}%")
        print(f"   Tests Ejecutados: {summary['total_tests']}")
        print(f"   Tiempo Total: {summary['total_execution_time']:.2f}s")
        print(
            f"   Quality Gate: {'‚úÖ APROBADO' if summary['quality_gate_passed'] else '‚ùå RECHAZADO'}"
        )

        # Mostrar recomendaciones cr√≠ticas
        if report["enterprise_analysis"]["recommendations"]:
            print("\nüéØ RECOMENDACIONES EMPRESARIALES:")
            for rec in report["enterprise_analysis"]["recommendations"][:3]:  # Top 3
                print(f"   {rec}")

        return 0 if summary["quality_gate_passed"] else 1

    # Calcular m√©tricas finales
    suite._calculate_enterprise_metrics()

    # Generar reporte final
    final_report = suite._generate_enterprise_report()
    suite.save_enterprise_report(final_report, args.format)

    # Mostrar m√©tricas finales
    metrics = suite.metrics
    print("\nüìà M√âTRICAS FINALES EMPRESARIALES:")
    print(f"   Tests Aprobados: {metrics.tests_passed}/{len(suite.results)}")
    print(f"   Tiempo Promedio: {metrics.average_test_duration:.2f}s")
    print(f"   Memoria M√°xima: {metrics.peak_memory_usage:.0f} MB")
    print(f"   CPU M√°ximo: {metrics.peak_cpu_usage:.1f}%")
    print(f"   Puntuaci√≥n Empresarial: {metrics.enterprise_quality_score:.1f}/100")

    # Retornar c√≥digo de salida empresarial
    success_rate = (metrics.tests_passed / len(suite.results)) * 100 if suite.results else 0
    return 0 if success_rate >= suite.enterprise_config["quality_gate_threshold"] else 1


if __name__ == "__main__":
    sys.exit(main())
