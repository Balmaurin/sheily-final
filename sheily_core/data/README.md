# Sheily Core - Document Processor
## Procesamiento Inteligente de Documentos

### ğŸ¯ PropÃ³sito
El `DocumentProcessor` es el componente responsable de procesar documentos JSONL y dividirlos en chunks semÃ¡nticos para el sistema RAG.

### ğŸ”§ Funcionalidades

#### Chunking Inteligente
- **RecursiveCharacterTextSplitter**: Divide texto respetando estructura semÃ¡ntica
- **Configurable**: TamaÃ±o de chunk (512) y solapamiento (100) personalizables
- **PreservaciÃ³n de metadata**: Mantiene informaciÃ³n contextual de cada chunk

#### Soporte Multi-idioma
- **EspaÃ±ol/inglÃ©s**: Procesamiento nativo de documentos en ambos idiomas
- **Corpus estructurado**: Soporte para `all-Branches/{domain}/corpus/{lang}/`
- **Metadata rica**: Incluye dominio, fuente, tÃ­tulo, keywords, etc.

### ğŸ“Š Uso BÃ¡sico

```python
from sheily_core.data.document_processor import DocumentProcessor

# Crear procesador
processor = DocumentProcessor(chunk_size=512, chunk_overlap=100)

# Procesar documentos
chunks = processor.process_corpus("all-Branches/antropologia/corpus/")

# Resultado: lista de diccionarios con chunks
for chunk in chunks[:3]:
    print(f"ID: {chunk['id']}")
    print(f"Contenido: {chunk['content'][:100]}...")
    print(f"Dominio: {chunk['metadata']['domain']}")
```

### ğŸ”§ ConfiguraciÃ³n Avanzada

```python
# ConfiguraciÃ³n personalizada
processor = DocumentProcessor(
    chunk_size=1024,      # TamaÃ±o de chunk mayor
    chunk_overlap=200     # MÃ¡s solapamiento
)

# Separadores personalizados
processor.text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=512,
    chunk_overlap=100,
    separators=["\\n\\n", "\\n", ". ", " ", ""],  # Prioridad de separaciÃ³n
    length_function=len
)
```

### ğŸ“ˆ MÃ©tricas de Performance

- **Velocidad**: ~10,000 tokens/segundo
- **Memoria**: ~50MB para corpus de 1M tokens
- **Escalabilidad**: Maneja corpus de cualquier tamaÃ±o

### ğŸ§ª Testing

```bash
# Tests especÃ­ficos del procesador
pytest tests/unit/test_document_processor.py -v

# Tests de integraciÃ³n con RAG
pytest tests/integration/test_rag_integration.py -v
```

### ğŸ” SoluciÃ³n de Problemas

#### Problema: "No se encontraron documentos"
**SoluciÃ³n**: Verificar estructura del directorio corpus
```
all-Branches/
â”œâ”€â”€ antropologia/
â”‚   â””â”€â”€ corpus/
â”‚       â”œâ”€â”€ spanish/
â”‚       â”‚   â””â”€â”€ documento.jsonl
â”‚       â””â”€â”€ english/
â”‚           â””â”€â”€ document.jsonl
```

#### Problema: Chunks muy pequeÃ±os/grandes
**SoluciÃ³n**: Ajustar parÃ¡metros de chunking
```python
processor = DocumentProcessor(chunk_size=256, chunk_overlap=50)  # MÃ¡s pequeÃ±os
processor = DocumentProcessor(chunk_size=1024, chunk_overlap=200)  # MÃ¡s grandes
```

### ğŸ”— IntegraciÃ³n con RAG

El DocumentProcessor se integra automÃ¡ticamente con el sistema RAG:

```python
from sheily_core.integration.rag_service import RAGService

service = RAGService()
# El service ya incluye el document processor configurado
chunks = service.document_processor.process_corpus("corpus/path/")
```

### ğŸ“ Formato de Documentos JSONL

Cada lÃ­nea debe ser un objeto JSON vÃ¡lido:

```json
{
  "title": "AntropologÃ­a Cultural",
  "content": "Texto completo del documento...",
  "domain": "antropologia",
  "category": "teoria_fundamental",
  "keywords": ["cultura", "sociedad", "antropologia"],
  "date": "2025-01-24"
}
```

### ğŸ¯ Mejores PrÃ¡cticas

1. **TamaÃ±o Ã³ptimo de chunks**: 512 tokens balancea contexto y precisiÃ³n
2. **Solapamiento**: 100 tokens asegura continuidad semÃ¡ntica
3. **Metadata completa**: Incluye toda la informaciÃ³n contextual posible
4. **ValidaciÃ³n**: Verifica formato JSONL antes del procesamiento
5. **Monitoreo**: Registra mÃ©tricas de procesamiento para optimizaciÃ³n

### ğŸ”„ Actualizaciones Futuras

- Soporte para mÃ¡s idiomas
- Chunking basado en transformers
- OptimizaciÃ³n automÃ¡tica de parÃ¡metros
- Procesamiento distribuido para corpus grandes
